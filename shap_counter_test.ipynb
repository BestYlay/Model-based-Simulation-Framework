{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76957d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Softwares\\Anaconda3\\envs\\kaggle\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1\n",
      "随机数种子已设置为: 42\n",
      "\n",
      "--- 当前模式: ANALYZE | 数据集: BRCA ---\n",
      "\n",
      "模型将使用以下特征:\n",
      "  - 数值: ['AGE', 'ERBB2', 'EVENT_DURATION', 'PTEN', 'STAGE 1', 'STAGE 3', 'STAGE 4', 'START_DATE', 'TP53', 'VALUE_NUMERIC']\n",
      "  - 分类: ['EVENT_TYPE', 'EVENT_SUBTYPE', 'VALUE_CATEGORICAL']\n",
      "\n",
      "--- 开始分析模式 ---\n",
      "\n",
      "================================================================================\n",
      "===============  开始分析第 1/3 折  ===============\n",
      "================================================================================\n",
      "  - 已从 './brca/fold_1/' 加载 Scaler 和 Vocabs。\n",
      "  - 已从 './brca/fold_1/best_model_fold_1.pth' 加载最佳模型。\n",
      "\n",
      "  [Fold 1] 最终验证集TD-AUC详情:\n",
      "     time    auc\n",
      " 275.3000 0.9023\n",
      " 289.8020 0.8962\n",
      " 304.3040 0.8965\n",
      " 318.8061 0.8966\n",
      " 333.3081 0.8950\n",
      " 347.8101 0.8985\n",
      " 362.3121 0.8988\n",
      " 376.8141 0.8988\n",
      " 391.3162 0.9022\n",
      " 405.8182 0.9030\n",
      " 420.3202 0.8996\n",
      " 434.8222 0.9006\n",
      " 449.3242 0.9008\n",
      " 463.8263 0.9002\n",
      " 478.3283 0.9043\n",
      " 492.8303 0.9039\n",
      " 507.3323 0.9055\n",
      " 521.8343 0.9042\n",
      " 536.3364 0.9064\n",
      " 550.8384 0.9028\n",
      " 565.3404 0.9078\n",
      " 579.8424 0.9099\n",
      " 594.3444 0.9127\n",
      " 608.8465 0.9113\n",
      " 623.3485 0.9117\n",
      " 637.8505 0.9129\n",
      " 652.3525 0.9133\n",
      " 666.8545 0.9131\n",
      " 681.3566 0.9144\n",
      " 695.8586 0.9167\n",
      " 710.3606 0.9122\n",
      " 724.8626 0.9129\n",
      " 739.3646 0.9105\n",
      " 753.8667 0.9107\n",
      " 768.3687 0.9121\n",
      " 782.8707 0.9126\n",
      " 797.3727 0.9151\n",
      " 811.8747 0.9160\n",
      " 826.3768 0.9148\n",
      " 840.8788 0.9152\n",
      " 855.3808 0.9175\n",
      " 869.8828 0.9180\n",
      " 884.3848 0.9169\n",
      " 898.8869 0.9204\n",
      " 913.3889 0.9198\n",
      " 927.8909 0.9219\n",
      " 942.3929 0.9223\n",
      " 956.8949 0.9205\n",
      " 971.3970 0.9216\n",
      " 985.8990 0.9226\n",
      "1000.4010 0.9217\n",
      "1014.9030 0.9233\n",
      "1029.4051 0.9254\n",
      "1043.9071 0.9267\n",
      "1058.4091 0.9271\n",
      "1072.9111 0.9302\n",
      "1087.4131 0.9261\n",
      "1101.9152 0.9285\n",
      "1116.4172 0.9272\n",
      "1130.9192 0.9286\n",
      "1145.4212 0.9271\n",
      "1159.9232 0.9252\n",
      "1174.4253 0.9248\n",
      "1188.9273 0.9260\n",
      "1203.4293 0.9271\n",
      "1217.9313 0.9264\n",
      "1232.4333 0.9256\n",
      "1246.9354 0.9260\n",
      "1261.4374 0.9262\n",
      "1275.9394 0.9262\n",
      "1290.4414 0.9286\n",
      "1304.9434 0.9276\n",
      "1319.4455 0.9267\n",
      "1333.9475 0.9276\n",
      "1348.4495 0.9270\n",
      "1362.9515 0.9260\n",
      "1377.4535 0.9271\n",
      "1391.9556 0.9276\n",
      "1406.4576 0.9272\n",
      "1420.9596 0.9280\n",
      "1435.4616 0.9277\n",
      "1449.9636 0.9276\n",
      "1464.4657 0.9273\n",
      "1478.9677 0.9283\n",
      "1493.4697 0.9303\n",
      "1507.9717 0.9305\n",
      "1522.4737 0.9307\n",
      "1536.9758 0.9330\n",
      "1551.4778 0.9326\n",
      "1565.9798 0.9325\n",
      "1580.4818 0.9326\n",
      "1594.9838 0.9324\n",
      "1609.4859 0.9317\n",
      "1623.9879 0.9317\n",
      "1638.4899 0.9303\n",
      "1652.9919 0.9293\n",
      "1667.4939 0.9295\n",
      "1681.9960 0.9291\n",
      "1696.4980 0.9297\n",
      "1711.0000 0.9278\n",
      "\n",
      "--- [Fold 1] 开始SHAP可解释性分析 ---\n",
      "  - 正在对验证集进行风险预测以筛选典型样本...\n",
      "  - 验证集平均预测风险: -1.1133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1018\u001b[39m\n\u001b[32m   1015\u001b[39m     all_folds_td_auc.append(final_td_auc_df)\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  [Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] 最终验证集TD-AUC详情:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfinal_td_auc_df.to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;250m \u001b[39mfloat_format=\u001b[33m'\u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m \u001b[43mrun_shap_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_df_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_numerical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_categorical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m prog_def = {\u001b[33m'\u001b[39m\u001b[33mEVENT_SUBTYPE\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mIMAGING_PROGRESSION\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVALUE_CATEGORICAL\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mY\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m   1022\u001b[39m cf_mod = {\u001b[33m'\u001b[39m\u001b[33mVALUE_CATEGORICAL\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mN\u001b[39m\u001b[33m'\u001b[39m}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 463\u001b[39m, in \u001b[36mrun_shap_analysis\u001b[39m\u001b[34m(model, train_df, val_df, numerical_cols, categorical_cols, fold_num, device, background_size, test_size)\u001b[39m\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_num, *pre_emb): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m(x_num, pre_embedded_categorical=\u001b[38;5;28mlist\u001b[39m(pre_emb))\n\u001b[32m    462\u001b[39m explainer = shap.DeepExplainer(SHAPWrapper(model_cpu), [background_numericals_t.float()] + background_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m shap_values_list = \u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_numericals_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# --- 5. 【【【核心修正】】】后处理SHAP值 ---\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# 1. 处理数值特征的SHAP值。\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# 原始形状: (batch, seq, num_numerical_features, output_dim=1)\u001b[39;00m\n\u001b[32m    468\u001b[39m shap_numerical = shap_values_list[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Softwares\\Anaconda3\\envs\\kaggle\\Lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:164\u001b[39m, in \u001b[36mDeepExplainer.shap_values\u001b[39m\u001b[34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs=\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order=\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m, check_additivity=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Softwares\\Anaconda3\\envs\\kaggle\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:189\u001b[39m, in \u001b[36mPyTorchDeep.shap_values\u001b[39m\u001b[34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[32m    188\u001b[39m feature_ind = model_output_ranks[j, i]\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m sample_phis = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.interim:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Softwares\\Anaconda3\\envs\\kaggle\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:120\u001b[39m, in \u001b[36mPyTorchDeep.gradient\u001b[39m\u001b[34m(self, idx, inputs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         grad = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m            \u001b[49m\u001b[43mselected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m             grad = grad.cpu().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Softwares\\Anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Softwares\\Anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Softwares\\Anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    288\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    293\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    296\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    297\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 导入所有必要的库\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import shap\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.metrics import cumulative_dynamic_auc\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from functools import partial\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# 设置显示选项和警告过滤器\n",
    "pd.set_option ('display.max_columns', None)\n",
    "pd.set_option ('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# Section 0: Setup and Preprocessing\n",
    "# =============================================================================\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"随机数种子已设置为: {seed_value}\")\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df_copy = df.copy().reset_index(drop=True)\n",
    "    if 'VALUE_NUMERIC' in df_copy.columns:\n",
    "        df_copy['VALUE_NUMERIC'] = df_copy['VALUE_NUMERIC'].fillna(0.0)\n",
    "    if 'VALUE_CATEGORICAL' in df_copy.columns:\n",
    "        df_copy['VALUE_CATEGORICAL'] = df_copy['VALUE_CATEGORICAL'].fillna('Missing')\n",
    "    return df_copy\n",
    "    \n",
    "# =============================================================================\n",
    "# Section 2-3: Data Handling and Feature Engineering\n",
    "# =============================================================================\n",
    "def encode_categorical_features_leakproof(train_df, val_df, categorical_cols):\n",
    "    vocab_mappings = {}\n",
    "    train_df_encoded = train_df.copy()\n",
    "    val_df_encoded = val_df.copy()\n",
    "    for col in categorical_cols:\n",
    "        train_df_encoded[col] = train_df_encoded[col].astype(str)\n",
    "        unique_vals = train_df_encoded[col].unique()\n",
    "        vocab = {val: i + 1 for i, val in enumerate(unique_vals)}\n",
    "        vocab['<PAD>'] = 0\n",
    "        vocab['<UNK>'] = len(vocab)\n",
    "        train_df_encoded[col + '_encoded'] = train_df_encoded[col].map(vocab)\n",
    "        val_df_encoded[col + '_encoded'] = val_df_encoded[col].astype(str).map(vocab)\n",
    "        val_df_encoded[col + '_encoded'].fillna(vocab['<UNK>'], inplace=True)\n",
    "        vocab_mappings[col] = {'vocab': vocab, 'vocab_size': len(vocab)}\n",
    "    return train_df_encoded, val_df_encoded, vocab_mappings\n",
    "\n",
    "def normalize_numerical_features_leakproof(train_df, val_df, numerical_cols):\n",
    "    scaler = StandardScaler()\n",
    "    train_df_normalized = train_df.copy()\n",
    "    val_df_normalized = val_df.copy()\n",
    "    if numerical_cols:\n",
    "        # 使用 .loc 避免 SettingWithCopyWarning\n",
    "        train_df_normalized.loc[:, numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n",
    "        val_df_normalized.loc[:, numerical_cols] = scaler.transform(val_df[numerical_cols])\n",
    "    return train_df_normalized, val_df_normalized, scaler\n",
    "\n",
    "class PatientSequenceDataset(Dataset):\n",
    "    def __init__(self, df, numerical_cols, categorical_cols_encoded):\n",
    "        self.sample_groups = {}\n",
    "        self.sample_ids = []\n",
    "        for sid, group in df.groupby('SAMPLE_ID'):\n",
    "            self.sample_ids.append(sid)\n",
    "            x_numerical = torch.tensor(group[numerical_cols].values, dtype=torch.float32)\n",
    "            x_categorical = {\n",
    "                col.replace('_encoded', ''): torch.tensor(group[col].values, dtype=torch.long)\n",
    "                for col in categorical_cols_encoded\n",
    "            }\n",
    "            label_time = torch.tensor(group['time'].iloc[0], dtype=torch.float32)\n",
    "            label_dead = torch.tensor(group['dead'].iloc[0], dtype=torch.float32)\n",
    "            self.sample_groups[sid] = (x_numerical, x_categorical, (label_time, label_dead))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sample_groups[self.sample_ids[idx]]\n",
    "\n",
    "def collate_fn_pad(batch):\n",
    "    (numericals, categoricals_list, labels) = zip(*batch)\n",
    "    padded_numericals = pad_sequence(numericals, batch_first=True, padding_value=0.0)\n",
    "    if categoricals_list and categoricals_list[0]:\n",
    "        categorical_keys = categoricals_list[0].keys()\n",
    "        categoricals_padded = {}\n",
    "        for key in categorical_keys:\n",
    "            sequences = [cat[key] for cat in categoricals_list]\n",
    "            categoricals_padded[key] = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        categoricals_padded = {}\n",
    "    label_times, label_deads = zip(*labels)\n",
    "    stacked_labels = (torch.stack(label_times), torch.stack(label_deads))\n",
    "    return padded_numericals, categoricals_padded, stacked_labels\n",
    "\n",
    "# =============================================================================\n",
    "# Section 4: Model, Loss, and Training/Evaluation Loops\n",
    "# =============================================================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SurvivalTransformer(nn.Module):\n",
    "    def __init__(self, vocab_sizes, embedding_dims, num_numerical_features, d_model=128, nhead=8, num_encoder_layers=4, dim_feedforward=256, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.categorical_keys = sorted(vocab_sizes.keys())\n",
    "        self.embedding_layers = nn.ModuleDict({\n",
    "            key: nn.Embedding(vocab_sizes[key], embedding_dims[key], padding_idx=0)\n",
    "            for key in self.categorical_keys\n",
    "        })\n",
    "        total_embedding_dim = sum(embedding_dims.values())\n",
    "        input_dim = num_numerical_features + total_embedding_dim\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout_prob)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, \n",
    "                dropout=dropout_prob, batch_first=True, activation='gelu'\n",
    "            ) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x_numerical, x_categorical=None, pre_embedded_categorical=None, return_attention=False):\n",
    "        if x_categorical is not None and self.categorical_keys:\n",
    "            src_key_padding_mask = (x_categorical[self.categorical_keys[0]] == 0)\n",
    "        elif pre_embedded_categorical is not None and pre_embedded_categorical:\n",
    "            src_key_padding_mask = (pre_embedded_categorical[0].sum(dim=-1) == 0)\n",
    "        else: # No categorical features\n",
    "            src_key_padding_mask = torch.zeros(x_numerical.shape[0], x_numerical.shape[1], dtype=torch.bool, device=x_numerical.device)\n",
    "\n",
    "        if pre_embedded_categorical is None:\n",
    "            embeds = [self.embedding_layers[key](x_categorical[key]) for key in self.categorical_keys] if self.categorical_keys else []\n",
    "        else:\n",
    "            embeds = pre_embedded_categorical\n",
    "\n",
    "        combined_features = torch.cat([x_numerical] + embeds, dim=2)\n",
    "        projected_features = self.input_projection(combined_features) * math.sqrt(self.d_model)\n",
    "        transformer_input = self.pos_encoder(projected_features)\n",
    "        \n",
    "        attention_weights = None\n",
    "        output = transformer_input\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            is_last_layer = (i == len(self.encoder_layers) - 1)\n",
    "            if is_last_layer and return_attention:\n",
    "                attn_output, attention_weights = layer.self_attn(\n",
    "                    output, output, output, \n",
    "                    key_padding_mask=src_key_padding_mask,\n",
    "                    need_weights=True,\n",
    "                    average_attn_weights=False\n",
    "                )\n",
    "                output = output + layer.dropout1(attn_output)\n",
    "                output = layer.norm1(output)\n",
    "                ff_output = layer.linear2(layer.dropout(layer.activation(layer.linear1(output))))\n",
    "                output = output + layer.dropout2(ff_output)\n",
    "                output = layer.norm2(output)\n",
    "            else:\n",
    "                output = layer(output, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        transformer_output = output\n",
    "        non_padding_mask = ~src_key_padding_mask\n",
    "        seq_lengths = non_padding_mask.sum(dim=1, keepdim=True)\n",
    "        masked_output = transformer_output * non_padding_mask.unsqueeze(-1)\n",
    "        summed_output = masked_output.sum(dim=1)\n",
    "        mean_output = summed_output / seq_lengths.clamp(min=1)\n",
    "        risk_score = self.fc(mean_output)\n",
    "        \n",
    "        if return_attention:\n",
    "            return risk_score, attention_weights\n",
    "        else:\n",
    "            return risk_score\n",
    "\n",
    "def cox_loss(risk_scores, times, events):\n",
    "    risk_scores = risk_scores.squeeze(-1)\n",
    "    sorted_indices = torch.argsort(times, descending=True)\n",
    "    risk_scores_sorted = risk_scores[sorted_indices]\n",
    "    events_sorted = events[sorted_indices]\n",
    "    log_risk_set_sum = torch.log(torch.cumsum(torch.exp(risk_scores_sorted), dim=0))\n",
    "    loss = -torch.sum(risk_scores_sorted[events_sorted.bool()] - log_risk_set_sum[events_sorted.bool()])\n",
    "    num_events = torch.sum(events)\n",
    "    if num_events > 0:\n",
    "        loss = loss / num_events\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, scaler, device):\n",
    "    model.train()\n",
    "    for batch_numerical_cpu, batch_categorical_cpu, (times_cpu, events_cpu) in dataloader:\n",
    "        batch_numerical = batch_numerical_cpu.to(device)\n",
    "        batch_categorical = {k: v.to(device) for k, v in batch_categorical_cpu.items()}\n",
    "        times, events = times_cpu.to(device), events_cpu.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=str(device).split(\":\")[0]):\n",
    "            risk_scores = model(batch_numerical, batch_categorical)\n",
    "            loss = loss_fn(risk_scores, times, events)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "def evaluate_model(model, dataloader, loss_fn, train_df_outcomes, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_risk_scores, all_times, all_events = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_numerical, batch_categorical, (times, events) in dataloader:\n",
    "            batch_numerical = batch_numerical.to(device)\n",
    "            batch_categorical = {k: v.to(device) for k, v in batch_categorical.items()}\n",
    "            with autocast(device_type=str(device).split(\":\")[0]):\n",
    "                risk_scores = model(batch_numerical, batch_categorical)\n",
    "                loss = loss_fn(risk_scores, times.to(device), events.to(device))\n",
    "            total_loss += loss.item()\n",
    "            all_risk_scores.append(risk_scores.cpu())\n",
    "            all_times.append(times.cpu())\n",
    "            all_events.append(events.cpu())\n",
    "\n",
    "    all_risk_scores_np = torch.cat(all_risk_scores).numpy()\n",
    "    all_times_np = torch.cat(all_times).numpy()\n",
    "    all_events_np = torch.cat(all_events).numpy()\n",
    "    \n",
    "    c_index = concordance_index(all_times_np, -all_risk_scores_np.squeeze(), all_events_np)\n",
    "    \n",
    "    train_outcomes_struct = np.array(list(zip(train_df_outcomes['dead'].astype(bool), train_df_outcomes['time'])), dtype=[('event', bool), ('time', float)])\n",
    "    val_outcomes_struct = np.array(list(zip(all_events_np.astype(bool), all_times_np)), dtype=[('event', bool), ('time', float)])\n",
    "    \n",
    "    event_times = train_df_outcomes[train_df_outcomes['dead'] == 1]['time']\n",
    "    if len(event_times) > 10:\n",
    "        min_time, max_time = np.quantile(event_times, [0.1, 0.9])\n",
    "        times_for_auc = np.linspace(min_time, max_time, 100) if max_time > min_time else [min_time]\n",
    "    else:\n",
    "        times_for_auc = np.quantile(train_df_outcomes['time'], [0.25, 0.5, 0.75])\n",
    "\n",
    "    try:\n",
    "        auc, mean_auc = cumulative_dynamic_auc(train_outcomes_struct, val_outcomes_struct, all_risk_scores_np.squeeze(), times_for_auc)\n",
    "        td_auc_df = pd.DataFrame({'time': times_for_auc, 'auc': auc})\n",
    "    except Exception as e:\n",
    "        print(f\"  - 警告: 计算 TD-AUC 失败。原因: {e}\")\n",
    "        mean_auc, td_auc_df = np.nan, None\n",
    "        \n",
    "    return total_loss / len(dataloader), c_index, mean_auc, td_auc_df\n",
    "\n",
    "# =============================================================================\n",
    "# Section 5: Plotting Function\n",
    "# =============================================================================\n",
    "def plot_fold_history(history, fold_num, output_dir=\"cv_plots\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 7))\n",
    "    \n",
    "    ax1.plot(epochs, history['train_loss'], 'bo-', label='Train Loss', markersize=3)\n",
    "    ax1.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss', markersize=3)\n",
    "    ax1.set_title(f'Fold {fold_num+1}: Loss Curve'); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Cox Loss'); ax1.legend(); ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, history['train_c_index'], 'bo-', label='Train C-Index', markersize=3)\n",
    "    ax2.plot(epochs, history['val_c_index'], 'ro-', label='Validation C-Index', markersize=3)\n",
    "    ax2.set_title(f'Fold {fold_num+1}: C-Index Curve'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('C-Index'); ax2.legend(); ax2.grid(True)\n",
    "    \n",
    "    ax3.plot(epochs, history['train_iauc'], 'bo-', label='Train iAUC', markersize=3)\n",
    "    ax3.plot(epochs, history['val_iauc'], 'ro-', label='Validation iAUC', markersize=3)\n",
    "    ax3.set_title(f'Fold {fold_num+1}: iAUC Curve'); ax3.set_xlabel('Epoch'); ax3.set_ylabel('iAUC'); ax3.legend(); ax3.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Fold {fold_num+1} Training & Validation History', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f\"fold_{fold_num+1}_training_history.png\")\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"图表已保存至: {save_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Section 7: SHAP & Counterfactual Analysis Functions\n",
    "# =============================================================================\n",
    "def visualize_attention_for_patient(\n",
    "    model, \n",
    "    patient_df, \n",
    "    numerical_cols, \n",
    "    categorical_cols, \n",
    "    sample_id, \n",
    "    fold_num, \n",
    "    death_status, \n",
    "    device,\n",
    "):\n",
    "    print(f\"  - 正在为样本 {sample_id} 生成注意力图...\")\n",
    "    model.to(device).eval()\n",
    "    categorical_cols_encoded = [c + '_encoded' for c in categorical_cols]\n",
    "    patient_dataset = PatientSequenceDataset(patient_df, numerical_cols, categorical_cols_encoded)\n",
    "    patient_loader = DataLoader(patient_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_pad)\n",
    "    batch_numerical, batch_categorical, _ = next(iter(patient_loader))\n",
    "    batch_numerical = batch_numerical.to(device)\n",
    "    batch_categorical = {k: v.to(device) for k, v in batch_categorical.items()}\n",
    "    with torch.no_grad():\n",
    "        _, attention_map = model(batch_numerical, batch_categorical, return_attention=True)\n",
    "    attention_map = attention_map.squeeze(0).cpu().mean(dim=0).numpy()\n",
    "    event_labels = [f\"T{i}: {st}\" for i, st in enumerate(patient_df['EVENT_SUBTYPE'])]\n",
    "    num_events = len(event_labels)\n",
    "    fig, ax = plt.subplots(figsize=(max(8, num_events/1.5), max(6, num_events/2)))\n",
    "    cax = ax.imshow(attention_map, cmap='viridis')\n",
    "    fig.colorbar(cax, label='Attention Weight')\n",
    "    ax.set_xticks(np.arange(num_events)); ax.set_yticks(np.arange(num_events))\n",
    "    ax.set_xticklabels(event_labels, rotation=90, ha=\"right\", fontsize=9)\n",
    "    ax.set_yticklabels(event_labels, fontsize=9)\n",
    "    ax.set_xlabel(\"Key (Attended To)\", fontsize=12); ax.set_ylabel(\"Query (Attending From)\", fontsize=12)\n",
    "    plt.title(f\"Attention Map for Sample {sample_id} (Status: {death_status}, Fold {fold_num})\", fontsize=14, pad=20)\n",
    "    plot_dir = f\"./{SET}/shap_plots_fold_{fold_num}\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    save_path = os.path.join(plot_dir, f\"attention_map_{sample_id}.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\"    - 注意力图已保存至: {save_path}\")\n",
    "\n",
    "def plot_shap_over_time(shap_values_tensor, input_tensor, sample_df, sample_id,\n",
    "                        feature_names, sample_ids_list, true_seq_lengths,\n",
    "                        fold_num, death_status, top_n_features=5):\n",
    "    print(f\"  - 正在为样本 {sample_id} 生成SHAP时序图...\")\n",
    "    try:\n",
    "        sample_idx = sample_ids_list.index(sample_id)\n",
    "    except (ValueError, IndexError):\n",
    "        print(f\"警告: 在SHAP分析的样本列表中未找到ID {sample_id}。\"); return\n",
    "    patient_shap = shap_values_tensor[sample_idx, :, :]\n",
    "    true_length = true_seq_lengths.get(sample_id, patient_shap.shape[0])\n",
    "    patient_shap = patient_shap[:true_length, :]\n",
    "    event_labels = [f\"T{i}: {st}\" for i, st in enumerate(sample_df[sample_df['SAMPLE_ID'] == sample_id]['EVENT_SUBTYPE'])]\n",
    "    total_importance = np.sum(np.abs(patient_shap), axis=0)\n",
    "    top_feature_indices = np.argsort(total_importance)[-top_n_features:]\n",
    "    plt.figure(figsize=(max(12, len(event_labels) * 0.8), 7))\n",
    "    colors = cm.get_cmap('tab10', len(top_feature_indices))\n",
    "    for i, f_idx in enumerate(top_feature_indices):\n",
    "        plt.plot(patient_shap[:, f_idx], marker='o', linestyle='-', label=feature_names[f_idx], color=colors(i))\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    plt.xticks(ticks=np.arange(len(event_labels)), labels=event_labels, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Event Sequence\"); plt.ylabel(\"SHAP Value (Impact on Risk)\")\n",
    "    plt.title(f\"Temporal SHAP Values for Sample {sample_id} (Status: {death_status}, Fold {fold_num})\")\n",
    "    plt.legend(title=\"Top Features\"); plt.grid(axis='x', linestyle=':', alpha=0.6); plt.tight_layout()\n",
    "    plot_dir = f\"./{SET}/shap_plots_fold_{fold_num}\"\n",
    "    save_path = os.path.join(plot_dir, f\"temporal_shap_{sample_id}.png\")\n",
    "    plt.savefig(save_path); plt.close()\n",
    "    print(f\"    - SHAP时序图已保存至: {save_path}\")\n",
    "\n",
    "def plot_waterfall(shap_values, feature_names, sample_id, death_status, save_dir, base_value, top_n=15):\n",
    "    explanation = shap.Explanation(values=shap_values, base_values=base_value, feature_names=feature_names)\n",
    "    plt.figure()\n",
    "    shap.plots.waterfall(explanation, max_display=top_n, show=False)\n",
    "    plt.title(f\"Aggregated SHAP Waterfall for Sample {sample_id} (Status: {death_status})\")\n",
    "    plt.tight_layout()\n",
    "    fname = os.path.join(save_dir, f\"waterfall_{sample_id}_{death_status}.png\")\n",
    "    plt.savefig(fname, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"    - 瀑布图已保存: {fname}\")\n",
    "\n",
    "def run_shap_analysis(model, train_df, val_df, numerical_cols, categorical_cols,\n",
    "                      fold_num, device='cuda', background_size=50, test_size=50):\n",
    "    \"\"\"\n",
    "    【最终修正版 v4】\n",
    "    执行完整的可解释性分析流程。\n",
    "    - 修复了因多个数值特征导致SHAP值张量构建失败的问题。\n",
    "    - 保留了智能筛选个案的功能。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- [Fold {fold_num}] 开始SHAP可解释性分析 ---\")\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    # --- 1. 对整个验证集进行风险预测，以用于样本筛选 ---\n",
    "    print(\"  - 正在对验证集进行风险预测以筛选典型样本...\")\n",
    "    categorical_cols_encoded = [c + '_encoded' for c in categorical_cols]\n",
    "    val_dataset_full = PatientSequenceDataset(val_df, numerical_cols, categorical_cols_encoded)\n",
    "    if not val_dataset_full:\n",
    "        print(\"警告: 验证集为空，跳过SHAP分析。\")\n",
    "        return\n",
    "    \n",
    "    val_loader_full = DataLoader(val_dataset_full, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad)\n",
    "    \n",
    "    all_val_risks = []\n",
    "    with torch.no_grad():\n",
    "        for num, cat, _ in val_loader_full:\n",
    "            risks = model(num.to(device), {k: v.to(device) for k, v in cat.items()})\n",
    "            all_val_risks.extend(risks.squeeze().cpu().numpy().flatten())\n",
    "    \n",
    "    risk_df = pd.DataFrame({'SAMPLE_ID': val_dataset_full.sample_ids, 'predicted_risk': all_val_risks})\n",
    "    val_outcomes_df = val_df[['SAMPLE_ID', 'dead']].drop_duplicates()\n",
    "    risk_df = pd.merge(risk_df, val_outcomes_df, on='SAMPLE_ID')\n",
    "    average_risk = risk_df['predicted_risk'].mean()\n",
    "    print(f\"  - 验证集平均预测风险: {average_risk:.4f}\")\n",
    "\n",
    "    # --- 2. 准备SHAP计算所需的数据子集 ---\n",
    "    background_samples = list(train_df['SAMPLE_ID'].unique()[:background_size])\n",
    "    background_df = train_df[train_df['SAMPLE_ID'].isin(background_samples)].copy()\n",
    "    test_samples_for_shap = list(risk_df['SAMPLE_ID'].unique()[:test_size])\n",
    "    test_df_shap = val_df[val_df['SAMPLE_ID'].isin(test_samples_for_shap)].copy()\n",
    "\n",
    "    if not background_samples or not test_samples_for_shap:\n",
    "        print(\"警告: 背景或测试样本为空，跳过SHAP分析。\")\n",
    "        return\n",
    "\n",
    "    # --- 3. 数据加载与批处理 ---\n",
    "    combined_df = pd.concat([background_df, test_df_shap])\n",
    "    combined_dataset = PatientSequenceDataset(combined_df, numerical_cols, categorical_cols_encoded)\n",
    "    if not combined_dataset: print(\"警告: 组合数据集为空，跳过SHAP。\"); return\n",
    "        \n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=len(combined_dataset), shuffle=False, collate_fn=collate_fn_pad)\n",
    "    all_numericals_padded, all_categoricals_padded, _ = next(iter(combined_loader))\n",
    "    combined_sample_order = combined_dataset.sample_ids\n",
    "\n",
    "    background_idx = [combined_sample_order.index(sid) for sid in background_samples if sid in combined_sample_order]\n",
    "    test_idx = [combined_sample_order.index(sid) for sid in test_samples_for_shap if sid in combined_sample_order]\n",
    "    if not background_idx or not test_idx: print(\"警告: 批次中无背景或测试样本，跳过SHAP。\"); return\n",
    "\n",
    "    background_numericals_t = all_numericals_padded[background_idx].cpu()\n",
    "    test_numericals_t = all_numericals_padded[test_idx].cpu()\n",
    "    background_categoricals_t = {k: v[background_idx].cpu() for k, v in all_categoricals_padded.items()}\n",
    "    test_categoricals_t = {k: v[test_idx].cpu() for k, v in all_categoricals_padded.items()}\n",
    "    \n",
    "    # --- 4. 预计算嵌入向量并运行SHAP ---\n",
    "    model_cpu = model.to('cpu').eval()\n",
    "    categorical_keys_sorted = model_cpu.categorical_keys\n",
    "    with torch.no_grad():\n",
    "        background_embeddings = [model_cpu.embedding_layers[key](background_categoricals_t[key]) for key in categorical_keys_sorted]\n",
    "        test_embeddings = [model_cpu.embedding_layers[key](test_categoricals_t[key]) for key in categorical_keys_sorted]\n",
    "\n",
    "    class SHAPWrapper(nn.Module):\n",
    "        def __init__(self, m): super().__init__(); self.m = m\n",
    "        def forward(self, x_num, *pre_emb): return self.m(x_num, pre_embedded_categorical=list(pre_emb))\n",
    "\n",
    "    explainer = shap.DeepExplainer(SHAPWrapper(model_cpu), [background_numericals_t.float()] + background_embeddings)\n",
    "    shap_values_list = explainer.shap_values([test_numericals_t.float()] + test_embeddings, check_additivity=False)\n",
    "    \n",
    "    # --- 5. 【【【核心修正】】】后处理SHAP值 ---\n",
    "    # 1. 处理数值特征的SHAP值。\n",
    "    # 原始形状: (batch, seq, num_numerical_features, output_dim=1)\n",
    "    shap_numerical = shap_values_list[0]\n",
    "    if shap_numerical.ndim == 4 and shap_numerical.shape[-1] == 1:\n",
    "        shap_numerical = np.squeeze(shap_numerical, axis=-1)  # 新形状: (batch, seq, num_numerical_features)\n",
    "    \n",
    "    # 2. 处理分类特征的SHAP值。\n",
    "    shap_categorical_summed = []\n",
    "    for s in shap_values_list[1:]:\n",
    "        # 原始形状: (batch, seq, embedding_dim, output_dim=1)\n",
    "        if s.ndim == 4 and s.shape[-1] == 1:\n",
    "            s = np.squeeze(s, axis=-1) # 新形状: (batch, seq, embedding_dim)\n",
    "        summed_s = np.sum(s, axis=-1, keepdims=True) # 新形状: (batch, seq, 1)\n",
    "        shap_categorical_summed.append(summed_s)\n",
    "\n",
    "    # 3. 沿特征维度连接所有数组\n",
    "    all_shap_tensors = [shap_numerical] + shap_categorical_summed\n",
    "    shap_values_temporal = np.concatenate(all_shap_tensors, axis=2)\n",
    "    \n",
    "    print(f\"  - SHAP值张量已成功构建，形状为: {shap_values_temporal.shape}\")\n",
    "    \n",
    "    # --- 6. 全局特征重要性绘图 ---\n",
    "    plot_dir = f\"./{SET}/shap_plots_fold_{fold_num}\"; os.makedirs(plot_dir, exist_ok=True)\n",
    "    feature_names = numerical_cols + categorical_keys_sorted\n",
    "    \n",
    "    # ... (全局绘图部分)\n",
    "    \n",
    "    # --- 7. 个案分析：根据风险筛选样本 ---\n",
    "    sample_ids_in_shap_calc = [combined_sample_order[i] for i in test_idx]\n",
    "    risk_df_subset = risk_df[risk_df['SAMPLE_ID'].isin(sample_ids_in_shap_calc)]\n",
    "    \n",
    "    high_risk_dead = risk_df_subset[(risk_df_subset['dead'] == 1) & (risk_df_subset['predicted_risk'] > average_risk)]['SAMPLE_ID'].tolist()\n",
    "    low_risk_alive = risk_df_subset[(risk_df_subset['dead'] == 0) & (risk_df_subset['predicted_risk'] < average_risk)]['SAMPLE_ID'].tolist()\n",
    "    \n",
    "    print(f\"\\n  - 找到 {len(high_risk_dead)} 个典型死亡样本 (死亡且风险 > 平均值)。\")\n",
    "    print(f\"  - 找到 {len(low_risk_alive)} 个典型存活样本 (存活且风险 < 平均值)。\")\n",
    "\n",
    "    per_sample_feature_sum = np.sum(shap_values_temporal, axis=1)\n",
    "    base_value = explainer.expected_value[0].item() if hasattr(explainer.expected_value, 'item') else explainer.expected_value[0]\n",
    "\n",
    "    if high_risk_dead:\n",
    "        sid = high_risk_dead[0]\n",
    "        risk_value = risk_df.loc[risk_df['SAMPLE_ID']==sid, 'predicted_risk'].item()\n",
    "        print(f\"\\n  分析典型死亡样本: {sid} (预测风险: {risk_value:.4f})\")\n",
    "        sample_pos = sample_ids_in_shap_calc.index(sid)\n",
    "        plot_waterfall(per_sample_feature_sum[sample_pos], feature_names, sid, \"Dead (High Risk)\", plot_dir, base_value)\n",
    "    \n",
    "    if low_risk_alive:\n",
    "        sid = low_risk_alive[0]\n",
    "        risk_value = risk_df.loc[risk_df['SAMPLE_ID']==sid, 'predicted_risk'].item()\n",
    "        print(f\"\\n  分析典型存活样本: {sid} (预测风险: {risk_value:.4f})\")\n",
    "        sample_pos = sample_ids_in_shap_calc.index(sid)\n",
    "        plot_waterfall(per_sample_feature_sum[sample_pos], feature_names, sid, \"Alive (Low Risk)\", plot_dir, base_value)\n",
    "\n",
    "    print(f\"\\n--- [Fold {fold_num}] SHAP可解释性分析完成 ---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# [通用反事实模拟函数]\n",
    "# =============================================================================\n",
    "\n",
    "def explain_single_df(model, df_to_explain, background_df, scaler, vocabs, numerical_cols, categorical_cols, device):\n",
    "    \"\"\"\n",
    "    【修正版 v2】为单个任意DataFrame计算SHAP值，用于反事实解释。\n",
    "    修复了因特征列不匹配导致的scaler.transform错误。\n",
    "    \"\"\"\n",
    "    if df_to_explain.empty:\n",
    "        return None, None\n",
    "    \n",
    "    # --- 【核心修正区域】---\n",
    "    # 1. 获取 scaler 在 fit 期间看到的特征名和顺序\n",
    "    expected_numerical_features = list(scaler.feature_names_in_)\n",
    "\n",
    "    # 2. 对齐列并应用变换 (对目标和背景数据都执行)\n",
    "    # 目标DataFrame\n",
    "    df_to_explain_norm = df_to_explain.copy()\n",
    "    for col in expected_numerical_features:\n",
    "        if col not in df_to_explain_norm.columns:\n",
    "            df_to_explain_norm[col] = 0.0\n",
    "    df_to_explain_norm.loc[:, expected_numerical_features] = scaler.transform(df_to_explain_norm[expected_numerical_features])\n",
    "\n",
    "    # 背景DataFrame\n",
    "    background_df_norm = background_df.copy()\n",
    "    for col in expected_numerical_features:\n",
    "        if col not in background_df_norm.columns:\n",
    "            background_df_norm[col] = 0.0\n",
    "    background_df_norm.loc[:, expected_numerical_features] = scaler.transform(background_df_norm[expected_numerical_features])\n",
    "    # --- 修正结束 ---\n",
    "    \n",
    "    # 编码部分保持不变\n",
    "    categorical_cols_encoded = []\n",
    "    for col in categorical_cols:\n",
    "        if col in vocabs:\n",
    "            encoded_col_name = col + '_encoded'\n",
    "            categorical_cols_encoded.append(encoded_col_name)\n",
    "            df_to_explain_norm[encoded_col_name] = df_to_explain_norm[col].astype(str).map(vocabs[col]['vocab']).fillna(vocabs[col]['vocab']['<UNK>'])\n",
    "            background_df_norm[encoded_col_name] = background_df_norm[col].astype(str).map(vocabs[col]['vocab']).fillna(vocabs[col]['vocab']['<UNK>'])\n",
    "\n",
    "    # 数据加载\n",
    "    combined_df = pd.concat([background_df_norm, df_to_explain_norm])\n",
    "    dataset = PatientSequenceDataset(combined_df, numerical_cols, categorical_cols_encoded)\n",
    "    if not dataset: return None, None\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, collate_fn=collate_fn_pad)\n",
    "    try:\n",
    "        numericals, categoricals, _ = next(iter(loader))\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "\n",
    "    # SHAP 计算 (与之前相同)\n",
    "    model_cpu = model.to('cpu').eval()\n",
    "    bg_tensors = [numericals[:-1].cpu().float()] + [model_cpu.embedding_layers[k](categoricals[k][:-1].cpu()) for k in model_cpu.categorical_keys]\n",
    "    test_tensors = [numericals[-1:].cpu().float()] + [model_cpu.embedding_layers[k](categoricals[k][-1:].cpu()) for k in model_cpu.categorical_keys]\n",
    "    \n",
    "    class SHAPWrapper(nn.Module):\n",
    "        def __init__(self, m): super().__init__(); self.m = m\n",
    "        def forward(self, x_num, *pre_emb): return self.m(x_num, pre_embedded_categorical=list(pre_emb))\n",
    "\n",
    "    explainer = shap.DeepExplainer(SHAPWrapper(model_cpu), bg_tensors)\n",
    "    shap_values_list = explainer.shap_values(test_tensors, check_additivity=False)\n",
    "    \n",
    "    # 后处理SHAP值 (使用稳健的concatenate)\n",
    "    shap_numerical = shap_values_list[0]\n",
    "    if shap_numerical.ndim == 4 and shap_numerical.shape[-1] == 1:\n",
    "        shap_numerical = np.squeeze(shap_numerical, axis=-1)\n",
    "    \n",
    "    shap_categorical_summed = []\n",
    "    for s in shap_values_list[1:]:\n",
    "        if s.ndim == 4 and s.shape[-1] == 1: s = np.squeeze(s, axis=-1)\n",
    "        summed_s = np.sum(s, axis=-1, keepdims=True)\n",
    "        shap_categorical_summed.append(summed_s)\n",
    "        \n",
    "    shap_temporal = np.concatenate([shap_numerical] + shap_categorical_summed, axis=2)\n",
    "    \n",
    "    aggregated_shap_values = np.sum(shap_temporal, axis=1).squeeze(0)\n",
    "    base_value = explainer.expected_value[0].item() if hasattr(explainer.expected_value, 'item') else explainer.expected_value[0]\n",
    "    \n",
    "    return aggregated_shap_values, base_value\n",
    "\n",
    "def _get_prediction_for_processed_df(df_processed, model, numerical_cols, categorical_cols_encoded, device):\n",
    "    \"\"\"\n",
    "    【修正版 v2】一个内部辅助函数，接收已经完全预处理好的DataFrame并返回预测值。\n",
    "    修复了模型和数据可能在不同设备上的问题。\n",
    "    \"\"\"\n",
    "    if df_processed.empty:\n",
    "        return np.nan\n",
    "        \n",
    "    temp_dataset = PatientSequenceDataset(df_processed, numerical_cols, categorical_cols_encoded)\n",
    "    if len(temp_dataset) == 0: return np.nan\n",
    "    \n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_pad)\n",
    "    \n",
    "    try:\n",
    "        batch_numerical, batch_categorical, _ = next(iter(temp_loader))\n",
    "    except StopIteration:\n",
    "        return np.nan\n",
    "\n",
    "    # --- 【核心修正区域】 ---\n",
    "    # 1. 确保模型在目标设备上\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2. 将数据移动到目标设备\n",
    "    batch_numerical = batch_numerical.to(device)\n",
    "    batch_categorical = {k: v.to(device) for k, v in batch_categorical.items()}\n",
    "    # --- 修正结束 ---\n",
    "\n",
    "    with torch.no_grad():\n",
    "        risk_score = model(batch_numerical, batch_categorical)\n",
    "        \n",
    "    return risk_score.squeeze().cpu().item()\n",
    "\n",
    "def _create_predictor_from_raw(model, numerical_cols, categorical_cols, scaler, vocabs, device):\n",
    "    \"\"\"\n",
    "    【修正版 v2】创建一个“预测函数”，它接收一个原始的DataFrame，在内部完成所有预处理步骤，\n",
    "    并确保在应用scaler之前，特征列与训练时完全一致。\n",
    "    \"\"\"\n",
    "    def predictor(raw_df):\n",
    "        if raw_df.empty:\n",
    "            return np.nan\n",
    "        \n",
    "        df_processed = raw_df.copy()\n",
    "        \n",
    "        # --- 【核心修正区域】---\n",
    "        # 1. 获取 scaler 在 fit 期间看到的特征名和顺序\n",
    "        expected_numerical_features = list(scaler.feature_names_in_)\n",
    "\n",
    "        # 2. 对齐列：确保 df_processed 拥有所有必需的数值列\n",
    "        for col in expected_numerical_features:\n",
    "            if col not in df_processed.columns:\n",
    "                # 如果缺少某个数值特征，用 0 填充\n",
    "                df_processed[col] = 0.0\n",
    "        \n",
    "        # 3. 按照 scaler 期望的顺序重新排列列，并只选择这些列进行变换\n",
    "        df_to_scale = df_processed[expected_numerical_features]\n",
    "        \n",
    "        # 4. 执行归一化\n",
    "        scaled_data = scaler.transform(df_to_scale)\n",
    "        df_processed.loc[:, expected_numerical_features] = scaled_data\n",
    "        # --- 修正结束 ---\n",
    "\n",
    "        # 编码部分保持不变\n",
    "        categorical_cols_encoded = []\n",
    "        for col in categorical_cols:\n",
    "            if col in vocabs:\n",
    "                encoded_col_name = col + '_encoded'\n",
    "                categorical_cols_encoded.append(encoded_col_name)\n",
    "                df_processed[encoded_col_name] = df_processed[col].astype(str).map(vocabs[col]['vocab'])\n",
    "                df_processed[encoded_col_name].fillna(vocabs[col]['vocab']['<UNK>'], inplace=True)\n",
    "\n",
    "        return _get_prediction_for_processed_df(\n",
    "            df_processed, model, numerical_cols, categorical_cols_encoded, device\n",
    "        )\n",
    "    return predictor\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def analyze_progression_impact_heterogeneity(results_df, val_df, static_features_list, fold_num):\n",
    "    \"\"\"\n",
    "    【修正版 v2】分析进展事件影响的异质性。\n",
    "    现在只对模型实际使用的静态特征进行分析和绘图。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- [Fold {fold_num}] 开始分析进展事件的影响异质性 ---\")\n",
    "    \n",
    "    if results_df is None or len(results_df) < 10:\n",
    "        print(\"  - 结果太少，无法进行有意义的异质性分析。跳过。\")\n",
    "        return\n",
    "\n",
    "    # 1. 根据风险差异定义“影响组” (与之前相同)\n",
    "    quantiles = results_df['risk_difference'].quantile([0.25, 0.75])\n",
    "    low_impact_threshold = quantiles[0.25]\n",
    "    high_impact_threshold = quantiles[0.75]\n",
    "\n",
    "    results_df['impact_group'] = pd.cut(\n",
    "        results_df['risk_difference'],\n",
    "        bins=[-np.inf, low_impact_threshold, high_impact_threshold, np.inf],\n",
    "        labels=['Low-Impact', 'Mid-Impact', 'High-Impact']\n",
    "    )\n",
    "    analysis_df = results_df[results_df['impact_group'].isin(['Low-Impact', 'High-Impact'])].copy()\n",
    "    \n",
    "    if analysis_df.empty:\n",
    "        print(\"  - 未能成功划分出高/低影响组。跳过。\")\n",
    "        return\n",
    "        \n",
    "    print(f\"  - 已将进展患者分为 {len(analysis_df[analysis_df['impact_group'] == 'Low-Impact'])} 名低影响组和 \"\n",
    "          f\"{len(analysis_df[analysis_df['impact_group'] == 'High-Impact'])} 名高影响组。\")\n",
    "\n",
    "    # 2. 合并患者在进展前的静态特征 (与之前相同)\n",
    "    static_features_df = val_df.sort_values('START_DATE').drop_duplicates(subset='PATIENT_ID', keep='first')\n",
    "    analysis_df = pd.merge(analysis_df, static_features_df, on='PATIENT_ID', how='left')\n",
    "\n",
    "    # 3. 【【【核心修正】】】\n",
    "    # 不再使用硬编码列表，而是直接使用传入的、模型实际使用的静态特征列表。\n",
    "    features_to_compare = [f for f in static_features_list if f in analysis_df.columns]\n",
    "    print(f\"  - 将对以下模型使用的静态特征进行比较: {features_to_compare}\")\n",
    "            \n",
    "    # 4. 为每个特征绘制箱线图 (与之前相同)\n",
    "    plot_dir = f\"./{SET}/impact_heterogeneity_plots_fold_{fold_num}\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    for feature in features_to_compare:\n",
    "        if analysis_df[feature].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(data=analysis_df, x='impact_group', y=feature, order=['Low-Impact', 'High-Impact'])\n",
    "        \n",
    "        group_low = analysis_df[analysis_df['impact_group'] == 'Low-Impact'][feature].dropna()\n",
    "        group_high = analysis_df[analysis_df['impact_group'] == 'High-Impact'][feature].dropna()\n",
    "        \n",
    "        if len(group_low) > 1 and len(group_high) > 1:\n",
    "            stat, p_value = ttest_ind(group_low, group_high, equal_var=False)\n",
    "            plt.title(f'Distribution of Pre-Progression \"{feature}\"\\nby Progression Impact Group (p-value: {p_value:.3f})')\n",
    "        else:\n",
    "            plt.title(f'Distribution of Pre-Progression \"{feature}\"\\nby Progression Impact Group')\n",
    "            \n",
    "        plt.xlabel(\"Impact Group\")\n",
    "        plt.ylabel(f\"Value of {feature}\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_path = os.path.join(plot_dir, f\"boxplot_{feature}_by_impact.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"  - 箱线图已保存: {save_path}\")\n",
    "\n",
    "def run_progression_counterfactual(model, train_df_norm, val_df, final_first_progression_df, progression_event_def, \n",
    "                                 counterfactual_change, scaler, vocabs, numerical_cols, \n",
    "                                 categorical_cols, fold_num, device='cpu'):\n",
    "    \"\"\"【最终修正版 v3】执行反事实模拟，生成对比SHAP图，并恢复了最终的风险差异直方图。\"\"\"\n",
    "    print(f\"\\n--- [Fold {fold_num}] 开始疾病进展反事实模拟与解释 ---\")\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    predictor = _create_predictor_from_raw(model, numerical_cols, categorical_cols, scaler, vocabs, device)\n",
    "    \n",
    "    pids_in_val = val_df['PATIENT_ID'].unique()\n",
    "    target_progression_df = final_first_progression_df[final_first_progression_df['PATIENT_ID'].isin(pids_in_val)]\n",
    "    if target_progression_df.empty:\n",
    "        print(\"  - 当前验证集中没有找到指定的进展患者。跳过。\")\n",
    "        return\n",
    "    \n",
    "    results, visualized_one = [], False\n",
    "\n",
    "    for _, row in target_progression_df.iterrows():\n",
    "        patient_id = row['PATIENT_ID']\n",
    "        patient_df = val_df[val_df['PATIENT_ID'] == patient_id].copy()\n",
    "        \n",
    "        factual_df = patient_df[patient_df['START_DATE'] <= row['START_DATE']]\n",
    "        history_before = patient_df[patient_df['START_DATE'] < row['START_DATE']]\n",
    "        prog_event_mask = (patient_df['START_DATE'] == row['START_DATE']) & (patient_df['EVENT_SUBTYPE'] == progression_event_def['EVENT_SUBTYPE'])\n",
    "        prog_event = patient_df[prog_event_mask]\n",
    "        if prog_event.empty: continue\n",
    "        \n",
    "        cf_event = prog_event.iloc[0:1].copy()\n",
    "        for col, val in counterfactual_change.items(): cf_event[col] = val\n",
    "        counterfactual_df = pd.concat([history_before, cf_event], ignore_index=True)\n",
    "        \n",
    "        factual_risk = predictor(factual_df)\n",
    "        counterfactual_risk = predictor(counterfactual_df)\n",
    "        \n",
    "        if not np.isnan(factual_risk) and not np.isnan(counterfactual_risk):\n",
    "            results.append({\n",
    "                'PATIENT_ID': patient_id, \n",
    "                'factual_risk': factual_risk, \n",
    "                'counterfactual_risk': counterfactual_risk, \n",
    "                'risk_difference': factual_risk - counterfactual_risk\n",
    "            })\n",
    "\n",
    "        if not visualized_one:\n",
    "            print(f\"  - 为患者 {patient_id} 生成反事实SHAP瀑布图...\")\n",
    "            feature_names = numerical_cols + sorted([c for c in categorical_cols if c in vocabs])\n",
    "            plot_dir = f\"./{SET}/counterfactual_plots_fold_{fold_num}\"\n",
    "            os.makedirs(plot_dir, exist_ok=True)\n",
    "            \n",
    "            background_for_cf = train_df_norm.sample(n=80, random_state=SEED)\n",
    "\n",
    "            factual_shap, base_val = explain_single_df(model, factual_df, background_for_cf, scaler, vocabs, numerical_cols, categorical_cols, device)\n",
    "            if factual_shap is not None:\n",
    "                title_suffix_factual = f\"Factual_Risk_{factual_risk:.3f}\"\n",
    "                plot_waterfall(factual_shap, feature_names, patient_id, title_suffix_factual, plot_dir, base_val)\n",
    "\n",
    "            cf_shap, base_val = explain_single_df(model, counterfactual_df, background_for_cf, scaler, vocabs, numerical_cols, categorical_cols, device)\n",
    "            if cf_shap is not None:\n",
    "                title_suffix_cf = f\"Counterfactual_Risk_{counterfactual_risk:.3f}\"\n",
    "                plot_waterfall(cf_shap, feature_names, patient_id, title_suffix_cf, plot_dir, base_val)\n",
    "            \n",
    "            visualized_one = True\n",
    "    \n",
    "    if not results:\n",
    "        print(\"  - 未能成功为任何进展患者生成模拟结果。\")\n",
    "        return\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    avg_risk_diff = results_df['risk_difference'].mean()\n",
    "    \n",
    "    print(\"\\n--- 疾病进展反事实模拟结果摘要 ---\")\n",
    "    print(results_df.head().to_string())\n",
    "    print(f\"\\n  平均风险差异 (真实进展风险 - 虚拟稳定风险): {avg_risk_diff:.4f}\")\n",
    "    \n",
    "    # --- 【【【核心修正：恢复直方图绘制代码】】】 ---\n",
    "    plot_dir = f\"./{SET}/counterfactual_plots_fold_{fold_num}\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=results_df, x='risk_difference', kde=True, bins=20)\n",
    "    plt.axvline(avg_risk_diff, color='red', linestyle='--', label=f'Average Difference: {avg_risk_diff:.4f}')\n",
    "    plt.title(f\"Distribution of Risk Differences (Fold {fold_num})\\nProgression vs. No Progression\")\n",
    "    plt.xlabel(\"Risk Difference (Factual Risk - Counterfactual Risk)\")\n",
    "    plt.ylabel(\"Number of Patients\")\n",
    "    plt.legend()\n",
    "    save_path = os.path.join(plot_dir, \"progression_risk_difference_histogram.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"\\n风险差异分布图已保存至: {save_path}\")\n",
    "    # --- 【修正结束】 ---\n",
    "    return results_df\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Block (MODIFIED FOR TRAIN/ANALYZE MODES)\n",
    "# =============================================================================\n",
    "# --- 0. 用户配置区域 ---\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 【【【【【 核心控制开关 】】】】】\n",
    "# 设置为 'train' 来运行训练并保存模型\n",
    "# 设置为 'analyze' 来加载已保存的模型并进行分析\n",
    "MODE = 'analyze' # <--- 修改这里 'train' 或 'analyze'\n",
    "\n",
    "SET = 'brca'\n",
    "data_filename = f'df_{SET}_landmarks.csv'\n",
    "print(f\"\\n--- 当前模式: {MODE.upper()} | 数据集: {SET.upper()} ---\")\n",
    "\n",
    "# --- 特征和数据配置 ---\n",
    "set_features = {\n",
    "    'brca': ['AGE', 'STAGE 1', 'STAGE 3', 'STAGE 4', 'PTEN', 'ERBB2', 'TP53'],\n",
    "    'crc': ['AGE', 'BLACK', 'STAGE 1', 'STAGE 2', 'STAGE 4', 'KRAS', 'BRAF'],\n",
    "    'nsclc': ['AGE', 'MALE', 'STAFE 1', 'STAGE 3', 'STAGE 4', 'PTEN', 'EGFR', 'TP53'],\n",
    "    'panc': ['AGE', 'MALE', 'STAGE 1', 'STAGE 4', 'KRAS', 'TP53'],\n",
    "    'prostate': ['AGE', 'BLACK', 'STAGE 4', 'PTEN', 'TP53']\n",
    "}\n",
    "CORE_FEATURES = ['START_DATE', 'VALUE_NUMERIC', 'EVENT_DURATION', 'EVENT_TYPE', 'EVENT_SUBTYPE', 'VALUE_CATEGORICAL']\n",
    "ALL_FEATURES = CORE_FEATURES + set_features[SET]\n",
    "STANDARDIZE_COLS = ['START_DATE', 'EVENT_DURATION', 'VALUE_NUMERIC', 'AGE']\n",
    "CATEGORICAL_COLS = ['EVENT_TYPE', 'EVENT_SUBTYPE', 'VALUE_CATEGORICAL']\n",
    "\n",
    "model_params = {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 1024, 'dropout_prob': 0.45}\n",
    "optimizer_params = {'lr': 8.1e-06, 'weight_decay': 0.00037}\n",
    "N_SPLITS = 3\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "data_filename = f'df_{SET}_landmarks.csv'\n",
    "\n",
    "# --- 1. 数据加载与准备 ---\n",
    "if not os.path.exists(data_filename):\n",
    "    print(f\"错误: 未找到数据文件 '{data_filename}'。\")\n",
    "else:\n",
    "    time_start = time.time()\n",
    "    df_full = pd.read_csv(data_filename)\n",
    "    if 'time' not in df_full.columns: df_full['time'] = df_full['stop']\n",
    "    df_processed = preprocess_dataframe(df_full)\n",
    "    \n",
    "    existing_features = [f for f in ALL_FEATURES if f in df_processed.columns]\n",
    "    final_categorical_cols = [c for c in CATEGORICAL_COLS if c in existing_features]\n",
    "    final_numerical_cols = sorted(list(set(existing_features) - set(final_categorical_cols)))\n",
    "    final_standardize_cols = [c for c in STANDARDIZE_COLS if c in final_numerical_cols]\n",
    "\n",
    "    print(f\"\\n模型将使用以下特征:\\n  - 数值: {final_numerical_cols}\\n  - 分类: {final_categorical_cols}\")\n",
    "\n",
    "    patient_outcomes = df_processed[['PATIENT_ID', 'dead']].drop_duplicates()\n",
    "    patient_ids = patient_outcomes['PATIENT_ID'].values\n",
    "    patient_dead_status = patient_outcomes['dead'].values\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # =========================================================================\n",
    "    #  TRAINING MODE\n",
    "    # =========================================================================\n",
    "    if MODE == 'train':\n",
    "        print(\"\\n--- 开始训练模式 ---\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(patient_ids, patient_dead_status)):\n",
    "            print(f\"\\n{'='*80}\\n===============  开始训练第 {fold+1}/{N_SPLITS} 折  ===============\\n{'='*80}\")\n",
    "            \n",
    "            # --- a. 数据准备 ---\n",
    "            train_patient_ids, val_patient_ids = patient_ids[train_idx], patient_ids[val_idx]\n",
    "            train_df = df_processed[df_processed['PATIENT_ID'].isin(train_patient_ids)].copy()\n",
    "            val_df = df_processed[df_processed['PATIENT_ID'].isin(val_patient_ids)].copy()\n",
    "            \n",
    "            train_df_encoded, val_df_encoded, vocabs = encode_categorical_features_leakproof(train_df, val_df, final_categorical_cols)\n",
    "            train_df_norm, val_df_norm, feature_scaler = normalize_numerical_features_leakproof(train_df_encoded, val_df_encoded, final_standardize_cols)\n",
    "            \n",
    "            # --- b. 模型初始化与训练 ---\n",
    "            categorical_cols_encoded = [c + '_encoded' for c in final_categorical_cols]\n",
    "            train_dataset = PatientSequenceDataset(train_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "            val_dataset = PatientSequenceDataset(val_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_pad)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad)\n",
    "            train_outcomes_df = train_df[['PATIENT_ID', 'dead', 'time']].drop_duplicates()\n",
    "            \n",
    "            vocab_sizes = {k: v['vocab_size'] for k, v in vocabs.items()}\n",
    "            embedding_dims = {k: min(50, (v['vocab_size'] // 2) + 1) for k, v in vocabs.items()}\n",
    "            model = SurvivalTransformer(vocab_sizes, embedding_dims, len(final_numerical_cols), **model_params).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), **optimizer_params, fused=torch.cuda.is_available())\n",
    "            grad_scaler = GradScaler()\n",
    "            \n",
    "            best_val_iauc = -1\n",
    "            \n",
    "            # --- c. 创建保存目录 ---\n",
    "            output_dir = f\"./{SET}/fold_{fold+1}/\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            best_model_path = os.path.join(output_dir, f\"best_model_fold_{fold+1}.pth\")\n",
    "            \n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                train_one_epoch(model, train_loader, optimizer, cox_loss, grad_scaler, DEVICE)\n",
    "                _, _, val_iauc, _ = evaluate_model(model, val_loader, cox_loss, train_outcomes_df, DEVICE)\n",
    "                if val_iauc > best_val_iauc:\n",
    "                    best_val_iauc = val_iauc\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                if (epoch + 1) % 20 == 0:\n",
    "                    print(f\"  Epoch {epoch+1:03d}/{NUM_EPOCHS} | Val iAUC: {val_iauc if not np.isnan(val_iauc) else 0:.4f} (Best: {best_val_iauc:.4f})\")\n",
    "\n",
    "            print(f\"--- 第 {fold+1} 折训练完成。最佳验证iAUC: {best_val_iauc:.4f} ---\")\n",
    "            print(f\"  - 模型已保存至: {best_model_path}\")\n",
    "            \n",
    "            # --- d. 【【核心新增】】保存预处理工具 ---\n",
    "            with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as f:\n",
    "                pickle.dump(feature_scaler, f)\n",
    "            with open(os.path.join(output_dir, 'vocabs.pkl'), 'wb') as f:\n",
    "                pickle.dump(vocabs, f)\n",
    "            print(f\"  - Scaler 和 Vocabs 已保存至: {output_dir}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    #  ANALYSIS MODE\n",
    "    # =========================================================================\n",
    "    elif MODE == 'analyze':\n",
    "        print(\"\\n--- 开始分析模式 ---\")\n",
    "        all_folds_td_auc, final_fold_metrics = [], []\n",
    "        \n",
    "        for fold in range(N_SPLITS):\n",
    "            print(f\"\\n{'='*80}\\n===============  开始分析第 {fold+1}/{N_SPLITS} 折  ===============\\n{'='*80}\")\n",
    "            \n",
    "            # --- a. 定义文件路径并检查文件是否存在 ---\n",
    "            input_dir = f\"./{SET}/fold_{fold+1}/\"\n",
    "            model_path = os.path.join(input_dir, f\"best_model_fold_{fold+1}.pth\")\n",
    "            scaler_path = os.path.join(input_dir, 'scaler.pkl')\n",
    "            vocabs_path = os.path.join(input_dir, 'vocabs.pkl')\n",
    "            \n",
    "            if not all(os.path.exists(p) for p in [model_path, scaler_path, vocabs_path]):\n",
    "                print(f\"错误: 在 '{input_dir}' 中缺少必要的模型或预处理文件。请先运行 'train' 模式。\")\n",
    "                continue\n",
    "\n",
    "            # --- b. 加载预处理工具 ---\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                feature_scaler = pickle.load(f)\n",
    "            with open(vocabs_path, 'rb') as f:\n",
    "                vocabs = pickle.load(f)\n",
    "            print(f\"  - 已从 '{input_dir}' 加载 Scaler 和 Vocabs。\")\n",
    "\n",
    "            # --- c. 数据准备 (与训练模式相同，但使用加载的工具) ---\n",
    "            # 注意：我们仍然需要执行一次数据划分来获取当前折的 train 和 val patient_ids\n",
    "            train_idx, val_idx = list(kf.split(patient_ids, patient_dead_status))[fold]\n",
    "            train_patient_ids, val_patient_ids = patient_ids[train_idx], patient_ids[val_idx]\n",
    "            \n",
    "            train_df = df_processed[df_processed['PATIENT_ID'].isin(train_patient_ids)].copy()\n",
    "            val_df = df_processed[df_processed['PATIENT_ID'].isin(val_patient_ids)].copy()\n",
    "            \n",
    "            # 应用加载的 vocabs 和 scaler\n",
    "            train_df_encoded, val_df_encoded, _ = encode_categorical_features_leakproof(train_df, val_df, final_categorical_cols)\n",
    "            train_df_norm, val_df_norm, _ = normalize_numerical_features_leakproof(train_df_encoded, val_df_encoded, final_standardize_cols)\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                PatientSequenceDataset(val_df_norm, final_numerical_cols, [c+'_encoded' for c in final_categorical_cols]),\n",
    "                batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad\n",
    "            )\n",
    "            train_outcomes_df = train_df[['PATIENT_ID', 'dead', 'time']].drop_duplicates()\n",
    "\n",
    "            # --- d. 加载模型 ---\n",
    "            vocab_sizes = {k: v['vocab_size'] for k, v in vocabs.items()}\n",
    "            embedding_dims = {k: min(50, (v['vocab_size'] // 2) + 1) for k, v in vocabs.items()}\n",
    "            \n",
    "            best_model = SurvivalTransformer(vocab_sizes, embedding_dims, len(final_numerical_cols), **model_params)\n",
    "            best_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "            best_model.to(DEVICE).eval()\n",
    "            print(f\"  - 已从 '{model_path}' 加载最佳模型。\")\n",
    "            \n",
    "            # --- e. 运行所有分析 ---\n",
    "            final_loss, final_c, final_iauc, final_td_auc_df = evaluate_model(best_model, val_loader, cox_loss, train_outcomes_df, DEVICE)\n",
    "            final_fold_metrics.append({'fold': fold + 1, 'val_loss': final_loss, 'val_c_index': final_c, 'val_iauc': final_iauc})\n",
    "            if final_td_auc_df is not None:\n",
    "                all_folds_td_auc.append(final_td_auc_df)\n",
    "                print(f\"\\n  [Fold {fold+1}] 最终验证集TD-AUC详情:\\n{final_td_auc_df.to_string(index=False, float_format='%.4f')}\")\n",
    "\n",
    "            run_shap_analysis(model=best_model, train_df=train_df_norm, val_df=val_df_norm, numerical_cols=final_numerical_cols, \n",
    "                              categorical_cols=final_categorical_cols, fold_num=fold+1, device='cpu', background_size=80, test_size=80)\n",
    "            \n",
    "            prog_def = {'EVENT_SUBTYPE': 'IMAGING_PROGRESSION', 'VALUE_CATEGORICAL': 'Y'}\n",
    "            cf_mod = {'VALUE_CATEGORICAL': 'N'}\n",
    "            prog_events = df_full[(df_full['EVENT_SUBTYPE'] == 'IMAGING_PROGRESSION') & (df_full['VALUE_CATEGORICAL'] == 'Y')]\n",
    "            first_prog_df = prog_events.loc[prog_events.groupby('PATIENT_ID')['START_DATE'].idxmin()]\n",
    "            \n",
    "            progression_results_df = run_progression_counterfactual(\n",
    "                model=best_model, train_df_norm=train_df_norm, val_df=val_df, \n",
    "                final_first_progression_df=first_prog_df, progression_event_def=prog_def,\n",
    "                counterfactual_change=cf_mod, scaler=feature_scaler, vocabs=vocabs,\n",
    "                numerical_cols=final_numerical_cols, categorical_cols=final_categorical_cols,\n",
    "                fold_num=fold+1, device='cpu'\n",
    "            )\n",
    "            \n",
    "            if progression_results_df is not None:\n",
    "                model_static_features = list(set(final_numerical_cols + final_categorical_cols) - set(CORE_FEATURES))\n",
    "                analyze_progression_impact_heterogeneity(\n",
    "                    results_df=progression_results_df, val_df=val_df,\n",
    "                    static_features_list=model_static_features, fold_num=fold+1\n",
    "                )\n",
    "        \n",
    "        # --- 3. 最终结果总结 (与之前相同) ---\n",
    "        print(f\"\\n{'='*80}\\n======================  最终交叉验证总结 ({SET.upper()})  ======================\\n{'='*80}\")\n",
    "        if final_fold_metrics:\n",
    "            results_df = pd.DataFrame(final_fold_metrics)\n",
    "            print(\"每折的最终验证集性能:\"); print(results_df.to_string(index=False))\n",
    "            print(\"\\n平均性能指标 (± 标准差):\")\n",
    "            print(f\"  - 验证集 Loss:   {results_df['val_loss'].mean():.4f} ± {results_df['val_loss'].std():.4f}\")\n",
    "            print(f\"  - 验证集 C-Index: {results_df['val_c_index'].mean():.4f} ± {results_df['val_c_index'].std():.4f}\")\n",
    "            print(f\"  - 验证集 iAUC:    {results_df['val_iauc'].mean():.4f} ± {results_df['val_iauc'].std():.4f}\")\n",
    "\n",
    "        if all_folds_td_auc:\n",
    "            # --- 1. 将所有折的AUC数据合并到一个大的DataFrame中 ---\n",
    "            # 这是一个关键步骤，它将每折的DataFrame列表整合成一个单一的、易于分析的DataFrame\n",
    "            combined_auc_df = pd.concat(all_folds_td_auc, ignore_index=True)\n",
    "            \n",
    "            # --- 2. 绘制平均时间依赖性AUC曲线图 ---\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            \n",
    "            # 使用seaborn的lineplot可以非常方便地自动计算并绘制均值和95%置信区间\n",
    "            sns.lineplot(data=combined_auc_df, x='time', y='auc', errorbar=('ci', 95), label='平均AUC (95% CI)')\n",
    "            \n",
    "            # 计算并获取一个总的平均AUC值，用于标题显示\n",
    "            total_mean_auc = combined_auc_df['auc'].mean()\n",
    "            \n",
    "            # 添加参考线和美化图表\n",
    "            plt.axhline(0.5, color='grey', linestyle='--', label='Random Guess (AUC=0.5)')\n",
    "            plt.title(f'Average Time-Dependent AUC for {SET.upper()} ({N_SPLITS}-Fold CV)\\nOverall Mean AUC = {total_mean_auc:.3f}', fontsize=15)\n",
    "            plt.xlabel('Time (Days)', fontsize=12)\n",
    "            plt.ylabel('AUC', fontsize=12)\n",
    "            plt.ylim(0.4, 1.0) # 设置一个对AUC合理的Y轴范围\n",
    "            plt.grid(True, linestyle=':', alpha=0.6)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # 保存图像到与数据集对应的文件夹中\n",
    "            save_path_fig = f\"./{SET}/average_td_auc_{SET}.png\"\n",
    "            plt.savefig(save_path_fig, dpi=300)\n",
    "            plt.show()\n",
    "            print(f\"\\n平均时间依赖性AUC曲线图已保存至: {save_path_fig}\")\n",
    "            \n",
    "            # --- 3. 打印按时间段划分的平均TD-AUC总结表格 ---\n",
    "            print(\"\\n按时间段划分的平均TD-AUC (± 标准差):\")\n",
    "            \n",
    "            # 定义时间区间（bins）和对应的标签\n",
    "            time_bins = [0, 365, 730, 1095, 1460, 1825, np.inf]\n",
    "            time_labels = ['0-1 Year', '1-2 Years', '2-3 Years', '3-4 Years', '4-5 Years', '>5 Years']\n",
    "            \n",
    "            # 使用pd.cut将连续的时间点划分到离散的时间段中\n",
    "            summary_df = combined_auc_df.copy()\n",
    "            summary_df['time_bin'] = pd.cut(summary_df['time'], bins=time_bins, labels=time_labels, right=False)\n",
    "            \n",
    "            # 按时间段分组，计算每个时间段内AUC的均值和标准差\n",
    "            auc_summary = summary_df.groupby('time_bin')['auc'].agg(['mean', 'std']).reset_index()\n",
    "            auc_summary = auc_summary.dropna(subset=['time_bin']) # 移除可能没有数据的区间\n",
    "\n",
    "            # 格式化输出，使其更易读\n",
    "            auc_summary['mean'] = auc_summary['mean'].map('{:.4f}'.format)\n",
    "            auc_summary['std'] = auc_summary['std'].map('{:.4f}'.format)\n",
    "            \n",
    "            print(auc_summary.to_string(index=False))\n",
    "\n",
    "    print(f\"\\n脚本总执行时间: {(time.time() - time_start) / 60:.2f} 分钟\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937916b",
   "metadata": {},
   "source": [
    "model_params = {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 1024, 'dropout_prob': 0.45}\n",
    "optimizer_params = {'lr': 8.1e-06, 'weight_decay': 0.00037}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
