{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c925757-cade-4b94-8b3a-feb16a6643a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.metrics import cumulative_dynamic_auc\n",
    "\n",
    "pd.set_option ('display.max_columns', None)\n",
    "pd.set_option ('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"随机数种子已设置为: {seed_value}\")\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df_copy = df.copy().reset_index(drop=True)\n",
    "    if 'VALUE_NUMERIC' in df_copy.columns:\n",
    "        df_copy['VALUE_NUMERIC'] = df_copy['VALUE_NUMERIC'].fillna(0.0)\n",
    "    if 'VALUE_CATEGORICAL' in df_copy.columns:\n",
    "        df_copy['VALUE_CATEGORICAL'] = df_copy['VALUE_CATEGORICAL'].fillna('Missing')\n",
    "    return df_copy\n",
    "\n",
    "def encode_categorical_features_leakproof(train_df, val_df, categorical_cols):\n",
    "    vocab_mappings = {}\n",
    "    train_df_encoded = train_df.copy()\n",
    "    val_df_encoded = val_df.copy()\n",
    "    for col in categorical_cols:\n",
    "        train_df_encoded[col] = train_df_encoded[col].astype(str)\n",
    "        unique_vals = train_df_encoded[col].unique()\n",
    "        vocab = {val: i + 1 for i, val in enumerate(unique_vals)}\n",
    "        vocab['<PAD>'] = 0\n",
    "        vocab['<UNK>'] = len(vocab)\n",
    "        train_df_encoded[col + '_encoded'] = train_df_encoded[col].map(vocab)\n",
    "        val_df_encoded[col + '_encoded'] = val_df_encoded[col].astype(str).map(vocab)\n",
    "        val_df_encoded[col + '_encoded'].fillna(vocab['<UNK>'], inplace=True)\n",
    "        vocab_mappings[col] = {'vocab': vocab, 'vocab_size': len(vocab)}\n",
    "    return train_df_encoded, val_df_encoded, vocab_mappings\n",
    "\n",
    "def normalize_numerical_features_leakproof(train_df, val_df, numerical_cols):\n",
    "    scaler = StandardScaler()\n",
    "    train_df_normalized = train_df.copy()\n",
    "    val_df_normalized = val_df.copy()\n",
    "    train_df_normalized[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n",
    "    val_df_normalized[numerical_cols] = scaler.transform(val_df[numerical_cols])\n",
    "    return train_df_normalized, val_df_normalized, scaler\n",
    "\n",
    "class PatientSequenceDataset(Dataset):\n",
    "    def __init__(self, df, numerical_cols, categorical_cols_encoded, device=None):\n",
    "        self.preferred_device = device\n",
    "        self.sample_groups = {}\n",
    "        self.sample_ids = []\n",
    "        for sid, group in df.groupby('SAMPLE_ID'):\n",
    "            self.sample_ids.append(sid)\n",
    "            x_numerical = torch.tensor(group[numerical_cols].values, dtype=torch.float32)\n",
    "            x_categorical = {\n",
    "                col.replace('_encoded', ''): torch.tensor(group[col].values, dtype=torch.long)\n",
    "                for col in categorical_cols_encoded\n",
    "            }\n",
    "            label_time = torch.tensor(group['time'].iloc[0], dtype=torch.float32)\n",
    "            label_dead = torch.tensor(group['dead'].iloc[0], dtype=torch.float32)\n",
    "            self.sample_groups[sid] = (x_numerical, x_categorical, (label_time, label_dead))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.sample_ids[idx]\n",
    "        return self.sample_groups[sid]\n",
    "\n",
    "def collate_fn_pad(batch):\n",
    "    (numericals, categoricals_list, labels) = zip(*batch)\n",
    "    padded_numericals = pad_sequence(numericals, batch_first=True, padding_value=0.0)\n",
    "    categorical_keys = categoricals_list[0].keys()\n",
    "    categoricals_padded = {}\n",
    "    for key in categorical_keys:\n",
    "        sequences = [cat[key] for cat in categoricals_list]\n",
    "        categoricals_padded[key] = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    label_times, label_deads = zip(*labels)\n",
    "    stacked_labels = (torch.stack(label_times), torch.stack(label_deads))\n",
    "    return padded_numericals, categoricals_padded, stacked_labels\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SurvivalTransformer(nn.Module):\n",
    "    def __init__(self, vocab_sizes, embedding_dims, num_numerical_features, d_model=128, nhead=8, num_encoder_layers=4, dim_feedforward=256, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.categorical_keys = sorted(vocab_sizes.keys())\n",
    "        self.embedding_layers = nn.ModuleDict({\n",
    "            key: nn.Embedding(vocab_sizes[key], embedding_dims[key], padding_idx=0)\n",
    "            for key in self.categorical_keys\n",
    "        })\n",
    "        total_embedding_dim = sum(embedding_dims.values())\n",
    "        input_dim = num_numerical_features + total_embedding_dim\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout_prob)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout_prob, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x_numerical, x_categorical):\n",
    "        src_key_padding_mask = (x_categorical[self.categorical_keys[0]] == 0)\n",
    "        embeds = [self.embedding_layers[key](x_categorical[key]) for key in self.categorical_keys]\n",
    "        combined_features = torch.cat([x_numerical] + embeds, dim=2)\n",
    "        projected_features = self.input_projection(combined_features) * math.sqrt(self.d_model)\n",
    "        pos_encoded_features = self.pos_encoder(projected_features)\n",
    "        transformer_output = self.transformer_encoder(pos_encoded_features, src_key_padding_mask=src_key_padding_mask)\n",
    "        non_padding_mask = ~src_key_padding_mask\n",
    "        seq_lengths = non_padding_mask.sum(dim=1, keepdim=True)\n",
    "        masked_output = transformer_output * non_padding_mask.unsqueeze(-1)\n",
    "        summed_output = masked_output.sum(dim=1)\n",
    "        mean_output = summed_output / seq_lengths.clamp(min=1)\n",
    "        risk_score = self.fc(mean_output)\n",
    "        return risk_score.squeeze(1)\n",
    "\n",
    "def cox_loss(risk_scores, times, events):\n",
    "    sorted_indices = torch.argsort(times, descending=True)\n",
    "    risk_scores_sorted = risk_scores[sorted_indices]\n",
    "    events_sorted = events[sorted_indices]\n",
    "    log_risk_set_sum = torch.log(torch.cumsum(torch.exp(risk_scores_sorted), dim=0))\n",
    "    loss = -torch.sum(risk_scores_sorted[events_sorted.bool()] - log_risk_set_sum[events_sorted.bool()])\n",
    "    num_events = torch.sum(events)\n",
    "    if num_events > 0:\n",
    "        loss = loss / num_events\n",
    "    return loss\n",
    "    \n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_numerical_cpu, batch_categorical_cpu, (times_cpu, events_cpu) in dataloader:\n",
    "        batch_numerical = batch_numerical_cpu.to(device)\n",
    "        batch_categorical = {k: v.to(device) for k, v in batch_categorical_cpu.items()}\n",
    "        times, events = times_cpu.to(device), events_cpu.to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=str(device).split(\":\")[0]):\n",
    "            risk_scores = model(batch_numerical, batch_categorical)\n",
    "            loss = loss_fn(risk_scores, times, events)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, loss_fn, device, eval_times):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_risk_scores, all_times, all_events = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_numerical, batch_categorical, (times, events) in dataloader:\n",
    "            batch_numerical = batch_numerical.to(device)\n",
    "            batch_categorical = {k: v.to(device) for k, v in batch_categorical.items()}\n",
    "            times, events = times.to(device), events.to(device)\n",
    "            \n",
    "            with autocast(device_type=str(device).split(\":\")[0]):\n",
    "                risk_scores = model(batch_numerical, batch_categorical)\n",
    "                loss = loss_fn(risk_scores, times, events)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_risk_scores.append(risk_scores.cpu())\n",
    "            all_times.append(times.cpu())\n",
    "            all_events.append(events.cpu())\n",
    "\n",
    "    risk_scores_raw_np = torch.cat(all_risk_scores).numpy() \n",
    "    times_np = torch.cat(all_times).numpy()\n",
    "    events_np = torch.cat(all_events).numpy().astype(bool)\n",
    "    \n",
    "    risk_for_sksurv = risk_scores_raw_np\n",
    "    risk_for_lifelines = -risk_scores_raw_np\n",
    "    \n",
    "    survival_train_for_auc = np.array([(e, t) for e, t in zip(events_np, times_np)], dtype=[('event', bool), ('time', float)])\n",
    "    survival_test_for_auc = survival_train_for_auc\n",
    "    \n",
    "    c_index = concordance_index(times_np, risk_for_lifelines, events_np) \n",
    "    \n",
    "    try:\n",
    "        auc, mean_auc = cumulative_dynamic_auc(\n",
    "            survival_train=survival_train_for_auc,\n",
    "            survival_test=survival_test_for_auc,\n",
    "            estimate=risk_for_sksurv, \n",
    "            times=eval_times\n",
    "        )\n",
    "        td_auc_results = {f\"TD-AUC@{int(t)}d\": auc_val for t, auc_val in zip(eval_times, auc)}\n",
    "    except Exception as e:\n",
    "        print(f\"  - 警告: TD-AUC 计算失败: {e}\")\n",
    "        td_auc_results = {f\"TD-AUC@{int(t)}d\": 0.0 for t in eval_times}\n",
    "        mean_auc = 0.0\n",
    "\n",
    "    return total_loss / len(dataloader), c_index, mean_auc, td_auc_results\n",
    "\n",
    "def objective(trial, df_for_tuning, eval_times, device):\n",
    "    print(f\"  [Trial {trial.number} on {device}] Starting...\")\n",
    "    time1 = time.time()\n",
    "    lr = trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True)\n",
    "    d_model = trial.suggest_categorical('d_model', [16, 32])\n",
    "    nhead = trial.suggest_categorical('nhead', [2, 4])\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 2, 4)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [128, 256])\n",
    "    dropout = trial.suggest_float('dropout', 0.4, 0.7)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 5e-5, 5e-3, log=True)\n",
    "\n",
    "    if d_model % nhead != 0:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    patient_ids = df_for_tuning['PATIENT_ID'].unique()\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    fold_metrics = []\n",
    "\n",
    "    fold_val_losses = []\n",
    "    fold_val_iaucs = []\n",
    "    fold_val_cindex = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(patient_ids)):\n",
    "        print(\"=\"*30)\n",
    "        print(f\"    [Trial {trial.number} on {device}] Fold {fold+1}/{N_SPLITS}...\")\n",
    "        \n",
    "        train_patient_ids = patient_ids[train_idx]\n",
    "        val_patient_ids = patient_ids[val_idx]\n",
    "        \n",
    "        train_df_fold = df_for_tuning[df_for_tuning['PATIENT_ID'].isin(train_patient_ids)]\n",
    "        val_df_fold = df_for_tuning[df_for_tuning['PATIENT_ID'].isin(val_patient_ids)]\n",
    "        \n",
    "        train_df_encoded, val_df_encoded, vocabs = encode_categorical_features_leakproof(train_df_fold, val_df_fold, final_categorical_cols)\n",
    "        train_df_norm, val_df_norm, _ = normalize_numerical_features_leakproof(train_df_encoded, val_df_encoded, final_standardize_cols)\n",
    "        \n",
    "        train_dataset = PatientSequenceDataset(train_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "        val_dataset = PatientSequenceDataset(val_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_pad, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad, pin_memory=True)\n",
    "        \n",
    "        vocab_sizes = {k: v['vocab_size'] for k, v in vocabs.items()}\n",
    "        model = SurvivalTransformer(\n",
    "            vocab_sizes=vocab_sizes,\n",
    "            embedding_dims=embedding_dims, \n",
    "            num_numerical_features=len(final_numerical_cols), \n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout_prob=dropout\n",
    "        ).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, fused=True)\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        best_val_metric = -1.0\n",
    "        patience_counter = 0\n",
    "\n",
    "        current_fold_val_losses = []\n",
    "        current_fold_val_iaucs = []\n",
    "        current_fold_val_cindex = []\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS_TUNE):\n",
    "            train_one_epoch(model, train_loader, optimizer, cox_loss, scaler, device)\n",
    "            val_loss, val_c, val_iauc, _ = evaluate_model(model, val_loader, cox_loss, device, eval_times)\n",
    "            current_metric = val_iauc\n",
    "            \n",
    "            current_fold_val_losses.append(val_loss)\n",
    "            current_fold_val_iaucs.append(val_iauc)\n",
    "            current_fold_val_cindex.append(val_c) \n",
    "            \n",
    "            if current_metric > best_val_metric:\n",
    "                best_val_metric = current_metric\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                 print(f\"  Epoch {epoch+1}, Loss: {val_loss:.4f}, C-Index: {val_c:.4f}, iAUC: {val_iauc:.4f}, Patience: {patience_counter}/{PATIENCE}\")\n",
    "\n",
    "            if patience_counter >= PATIENCE:\n",
    "                break\n",
    "        \n",
    "        fold_metrics.append(best_val_metric)\n",
    "        fold_val_losses.append(current_fold_val_losses)\n",
    "        fold_val_iaucs.append(current_fold_val_iaucs)\n",
    "        fold_val_cindex.append(current_fold_val_cindex)\n",
    "\n",
    "    mean_val_metric = np.mean(fold_metrics)\n",
    "    time2 = time.time()\n",
    "    print(f\"  [Trial {trial.number} on {device}] Finished in {time2 - time1:.2f}s. Mean Val iAUC: {mean_val_metric:.5f}\")\n",
    "\n",
    "    plot_dir = \"./plot\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    trial_plot_dir = os.path.join(plot_dir, f\"Trial {trial.number}\")\n",
    "    os.makedirs(trial_plot_dir, exist_ok=True)\n",
    "\n",
    "    for fold_idx in range(N_SPLITS):\n",
    "        epochs_range = range(1, len(fold_val_losses[fold_idx]) + 1)\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 5))\n",
    "        \n",
    "        # Loss曲线\n",
    "        ax1.plot(epochs_range, fold_val_losses[fold_idx], label='Validation Loss', color='blue')\n",
    "        ax1.set_title(f'Fold {fold_idx+1} Loss Curve')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Cox Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # iAUC曲线\n",
    "        ax2.plot(epochs_range, fold_val_iaucs[fold_idx], label='Validation iAUC', color='red')\n",
    "        ax2.set_title(f'Fold {fold_idx+1} iAUC Curve')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('iAUC')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        # C-Index曲线\n",
    "        ax3.plot(epochs_range, fold_val_cindex[fold_idx], label='Validation C-Index', color='green')\n",
    "        ax3.set_title(f'Fold {fold_idx+1} C-Index Curve')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('C-Index')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        plt.suptitle(f'Trial {trial.number} Performance on {device}')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(os.path.join(trial_plot_dir, f\"fold_{fold_idx+1}_performance.png\"))\n",
    "        plt.close(fig)\n",
    "        \n",
    "    print(f\"  [Trial {trial.number} on {device}] Plots saved to {trial_plot_dir}\")\n",
    "\n",
    "    return mean_val_metric\n",
    "\n",
    "def train_final_model_and_plot(best_params, df_train, df_test, eval_times, device):\n",
    "    print(f\"\\n--- 步骤 4: 在 {device} 上训练最终模型并评估 ---\")\n",
    "    \n",
    "    train_df_encoded, test_df_encoded, vocabs = encode_categorical_features_leakproof(df_train, df_test, final_categorical_cols)\n",
    "    train_df_norm, test_df_norm, scaler = normalize_numerical_features_leakproof(train_df_encoded, test_df_encoded, final_standardize_cols)\n",
    "\n",
    "    train_dataset = PatientSequenceDataset(train_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "    test_dataset = PatientSequenceDataset(test_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_pad)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad)\n",
    "\n",
    "    vocab_sizes = {k: v['vocab_size'] for k, v in vocabs.items()}\n",
    "    model = SurvivalTransformer(\n",
    "        vocab_sizes=vocab_sizes, \n",
    "        embedding_dims=embedding_dims, \n",
    "        num_numerical_features=len(final_numerical_cols),\n",
    "        d_model=best_params['d_model'], \n",
    "        nhead=best_params['nhead'], \n",
    "        num_encoder_layers=best_params['num_encoder_layers'], \n",
    "        dim_feedforward=best_params['dim_feedforward'], \n",
    "        dropout_prob=best_params['dropout']\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'], fused=True)\n",
    "    grad_scaler = GradScaler()\n",
    "    \n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_c': [], 'test_c': []}\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS_FINAL):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, cox_loss, grad_scaler, device)\n",
    "        train_loss_eval, train_c, _, _ = evaluate_model(model, train_loader, cox_loss, device, eval_times)\n",
    "        test_loss, test_c, test_iauc, test_td_auc_detailed = evaluate_model(model, test_loader, cox_loss, device, eval_times)\n",
    "        \n",
    "        history['train_loss'].append(train_loss_eval)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_c'].append(train_c)\n",
    "        history['test_c'].append(test_c)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS_FINAL} | \"\n",
    "                  f\"Train Loss: {train_loss_eval:.4f}, Test Loss: {test_loss:.4f} | \"\n",
    "                  f\"Train C-Idx: {train_c:.4f}, Test C-Idx: {test_c:.4f}\")\n",
    "\n",
    "    print(\"\\n--- 最终模型在独立测试集上的性能 ---\")\n",
    "    print(f\"  - 最终测试集 C-Index: {test_c:.4f}\")\n",
    "    print(f\"  - 最终测试集 iAUC: {test_iauc:.4f}\")\n",
    "    for k, v in test_td_auc_detailed.items():\n",
    "        print(f\"    - {k}: {v:.4f}\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    epochs = range(1, NUM_EPOCHS_FINAL + 1)\n",
    "\n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(epochs, history['test_loss'], label='Test Loss')\n",
    "    ax1.set_title('Loss Curve')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Cox Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(epochs, history['train_c'], label='Train C-Index')\n",
    "    ax2.plot(epochs, history['test_c'], label='Test C-Index')\n",
    "    ax2.set_title('C-Index Curve')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('C-Index')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.suptitle('Model Train History')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(\"final_model_training_history.png\")\n",
    "    print(\"\\n训练历史图表已保存至 'final_model_training_history.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "GPU_ID = 0  \n",
    "# --- 主程序开始 ---\n",
    "time_start = time.time()\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = f\"cuda:{GPU_ID}\"\n",
    "print(f\"####################################################\")\n",
    "print(f\"###   本笔记本将运行在主设备: {DEVICE}   ###\")\n",
    "print(f\"####################################################\\n\")\n",
    "if not os.path.exists('df_prostate_landmarks.csv'):\n",
    "    print(\"错误: 未找到数据文件 'df_brca_landmarks.csv'。请将数据文件放在与笔记本相同的目录中。\")\n",
    "else:\n",
    "    ALL_FEATURES = [\n",
    "        'START_DATE', 'VALUE_NUMERIC', 'EVENT_DURATION', \n",
    "        'EVENT_TYPE', 'EVENT_SUBTYPE', 'VALUE_CATEGORICAL', \n",
    "        'AGE', 'BLACK', 'STAGE 4', 'PTEN', 'TP53'\n",
    "    ]\n",
    "    STANDARDIZE_COLS = ['START_DATE', 'EVENT_DURATION', 'VALUE_NUMERIC', 'AGE']\n",
    "    CATEGORICAL_COLS = ['EVENT_TYPE', 'EVENT_SUBTYPE', 'VALUE_CATEGORICAL']\n",
    "    embedding_dims = {'EVENT_TYPE': 8, 'EVENT_SUBTYPE': 10, 'VALUE_CATEGORICAL': 12}\n",
    "\n",
    "    TEST_SET_SIZE = 0.3\n",
    "    BATCH_SIZE = 64\n",
    "    N_SPLITS = 5\n",
    "    PATIENCE = 5\n",
    "    NUM_EPOCHS_TUNE = 60\n",
    "    NUM_EPOCHS_FINAL = 80\n",
    "    N_TRIALS_PER_GPU = 15 \n",
    "\n",
    "    print(\"--- 步骤 1: 加载和准备数据 ---\")\n",
    "    df_full = pd.read_csv('df_crc_landmarks.csv')\n",
    "\n",
    "    EVALUATION_TIMES = np.array([365, 730, 1095, 1460, 1825]) \n",
    "    existing_features = [f for f in ALL_FEATURES if f in df_full.columns]\n",
    "    df_processed = preprocess_dataframe(df_full[['SAMPLE_ID', 'PATIENT_ID', 'entry', 'stop', 'dead', 'time'] + existing_features])\n",
    "\n",
    "    final_categorical_cols = [c for c in CATEGORICAL_COLS if c in df_processed.columns]\n",
    "    final_numerical_cols = sorted(list(set(existing_features) - set(final_categorical_cols)))\n",
    "    final_standardize_cols = [c for c in STANDARDIZE_COLS if c in final_numerical_cols]\n",
    "    categorical_cols_encoded = [c + '_encoded' for c in final_categorical_cols]\n",
    "\n",
    "    print(f\"\\n--- 步骤 2: 划分训练集 ({1-TEST_SET_SIZE:.0%}) 和最终测试集 ({TEST_SET_SIZE:.0%}) ---\")\n",
    "    all_patient_ids = df_processed['PATIENT_ID'].unique()\n",
    "    train_patient_ids, test_patient_ids = train_test_split(all_patient_ids, test_size=TEST_SET_SIZE, random_state=SEED)\n",
    "    df_train = df_processed[df_processed['PATIENT_ID'].isin(train_patient_ids)].copy()\n",
    "    df_test = df_processed[df_processed['PATIENT_ID'].isin(test_patient_ids)].copy()\n",
    "    print(f\"训练集患者数: {len(train_patient_ids)}, 测试集患者数: {len(test_patient_ids)}\")\n",
    "\n",
    "    print(f\"\\n--- 步骤 3: 使用Optuna进行超参数搜索 (本进程贡献自 {DEVICE}) ---\")\n",
    "    db_name = \"optuna_parallel_study.db\"\n",
    "    storage_name = f\"sqlite:///{db_name}\"\n",
    "    study_name = \"survival_study_jupyter\"\n",
    "\n",
    "    if GPU_ID == 0 and os.path.exists(db_name):\n",
    "        print(f\"由主进程({DEVICE})检测到旧数据库，正在删除: {db_name}\")\n",
    "        os.remove(db_name)\n",
    "    time.sleep(2)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_name,\n",
    "        direction=\"maximize\",\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, df_train, EVALUATION_TIMES, DEVICE),\n",
    "        n_trials=N_TRIALS_PER_GPU \n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- 进程 {DEVICE} 的 {N_TRIALS_PER_GPU} 次 Optuna 试验已完成 ---\")\n",
    "\n",
    "    if GPU_ID == 0:\n",
    "        print(\"\\n\\n==========================================================\")\n",
    "        print(\"=== 主进程({DEVICE})等待其他进程完成... ===\")\n",
    "        print(\"==========================================================\")\n",
    "\n",
    "        total_trials_expected = N_TRIALS_PER_GPU * torch.cuda.device_count()\n",
    "        while len(study.get_trials()) < total_trials_expected:\n",
    "            print(f\"当前完成 {len(study.get_trials())}/{total_trials_expected} trials. 等待30秒...\")\n",
    "            time.sleep(30)\n",
    "            \n",
    "        study = optuna.load_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_name\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Optuna 全局搜索完成 ---\")\n",
    "        print(f\"数据库中的总试验次数: {len(study.trials)}\")\n",
    "        print(f\"最佳试验 (iAUC): {study.best_value:.5f}\")\n",
    "        print(\"最佳超参数:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        optuna_results_df = study.trials_dataframe()\n",
    "        optuna_results_df.to_csv(\"optuna_search_results.csv\", index=False)\n",
    "        print(\"\\n所有Optuna试验的详细结果已保存至 'optuna_search_results.csv'\")\n",
    "        train_final_model_and_plot(study.best_params, df_train, df_test, EVALUATION_TIMES, DEVICE)\n",
    "\n",
    "        time_end = time.time()\n",
    "        print(f\"\\n脚本总执行时间: {(time_end - time_start) / 60:.2f} 分钟\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
