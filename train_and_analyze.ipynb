{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daecabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import shap\n",
    "from lifelines.utils import concordance_index\n",
    "from sksurv.metrics import cumulative_dynamic_auc\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from functools import partial\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "\n",
    "pd.set_option ('display.max_columns', None)\n",
    "pd.set_option ('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5319a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"随机数种子已设置为: {seed_value}\")\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df_copy = df.copy().reset_index(drop=True)\n",
    "    if 'VALUE_NUMERIC' in df_copy.columns:\n",
    "        df_copy['VALUE_NUMERIC'] = df_copy['VALUE_NUMERIC'].fillna(0.0)\n",
    "    if 'VALUE_CATEGORICAL' in df_copy.columns:\n",
    "        df_copy['VALUE_CATEGORICAL'] = df_copy['VALUE_CATEGORICAL'].fillna('Missing')\n",
    "    return df_copy\n",
    "    \n",
    "def encode_categorical_features_leakproof(train_df, val_df, categorical_cols):\n",
    "    vocab_mappings = {}\n",
    "    train_df_encoded = train_df.copy()\n",
    "    val_df_encoded = val_df.copy()\n",
    "    for col in categorical_cols:\n",
    "        train_df_encoded[col] = train_df_encoded[col].astype(str)\n",
    "        unique_vals = train_df_encoded[col].unique()\n",
    "        vocab = {val: i + 1 for i, val in enumerate(unique_vals)}\n",
    "        vocab['<PAD>'] = 0\n",
    "        vocab['<UNK>'] = len(vocab)\n",
    "        train_df_encoded[col + '_encoded'] = train_df_encoded[col].map(vocab)\n",
    "        val_df_encoded[col + '_encoded'] = val_df_encoded[col].astype(str).map(vocab)\n",
    "        val_df_encoded[col + '_encoded'].fillna(vocab['<UNK>'], inplace=True)\n",
    "        vocab_mappings[col] = {'vocab': vocab, 'vocab_size': len(vocab)}\n",
    "    return train_df_encoded, val_df_encoded, vocab_mappings\n",
    "\n",
    "def normalize_numerical_features_leakproof(train_df, val_df, numerical_cols):\n",
    "    scaler = StandardScaler()\n",
    "    train_df_normalized = train_df.copy()\n",
    "    val_df_normalized = val_df.copy()\n",
    "    if numerical_cols:\n",
    "        train_df_normalized.loc[:, numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n",
    "        val_df_normalized.loc[:, numerical_cols] = scaler.transform(val_df[numerical_cols])\n",
    "    return train_df_normalized, val_df_normalized, scaler\n",
    "\n",
    "class PatientSequenceDataset(Dataset):\n",
    "    def __init__(self, df, numerical_cols, categorical_cols_encoded):\n",
    "        self.sample_groups = {}\n",
    "        self.sample_ids = []\n",
    "        for sid, group in df.groupby('SAMPLE_ID'):\n",
    "            self.sample_ids.append(sid)\n",
    "            x_numerical = torch.tensor(group[numerical_cols].values, dtype=torch.float32)\n",
    "            x_categorical = {\n",
    "                col.replace('_encoded', ''): torch.tensor(group[col].values, dtype=torch.long)\n",
    "                for col in categorical_cols_encoded\n",
    "            }\n",
    "            label_time = torch.tensor(group['time'].iloc[0], dtype=torch.float32)\n",
    "            label_dead = torch.tensor(group['dead'].iloc[0], dtype=torch.float32)\n",
    "            self.sample_groups[sid] = (x_numerical, x_categorical, (label_time, label_dead))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sample_groups[self.sample_ids[idx]]\n",
    "\n",
    "def collate_fn_pad(batch):\n",
    "    (numericals, categoricals_list, labels) = zip(*batch)\n",
    "    padded_numericals = pad_sequence(numericals, batch_first=True, padding_value=0.0)\n",
    "    if categoricals_list and categoricals_list[0]:\n",
    "        categorical_keys = categoricals_list[0].keys()\n",
    "        categoricals_padded = {}\n",
    "        for key in categorical_keys:\n",
    "            sequences = [cat[key] for cat in categoricals_list]\n",
    "            categoricals_padded[key] = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        categoricals_padded = {}\n",
    "    label_times, label_deads = zip(*labels)\n",
    "    stacked_labels = (torch.stack(label_times), torch.stack(label_deads))\n",
    "    return padded_numericals, categoricals_padded, stacked_labels\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SurvivalTransformer(nn.Module):\n",
    "    def __init__(self, vocab_sizes, embedding_dims, num_numerical_features, d_model=128, nhead=8, num_encoder_layers=4, dim_feedforward=256, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.categorical_keys = sorted(vocab_sizes.keys())\n",
    "        self.embedding_layers = nn.ModuleDict({\n",
    "            key: nn.Embedding(vocab_sizes[key], embedding_dims[key], padding_idx=0)\n",
    "            for key in self.categorical_keys\n",
    "        })\n",
    "        total_embedding_dim = sum(embedding_dims.values())\n",
    "        input_dim = num_numerical_features + total_embedding_dim\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout_prob)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, \n",
    "                dropout=dropout_prob, batch_first=True, activation='gelu'\n",
    "            ) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x_numerical, x_categorical=None, pre_embedded_categorical=None, return_attention=False):\n",
    "        if x_categorical is not None and self.categorical_keys:\n",
    "            src_key_padding_mask = (x_categorical[self.categorical_keys[0]] == 0)\n",
    "        elif pre_embedded_categorical is not None and pre_embedded_categorical:\n",
    "            src_key_padding_mask = (pre_embedded_categorical[0].sum(dim=-1) == 0)\n",
    "        else: \n",
    "            src_key_padding_mask = torch.zeros(x_numerical.shape[0], x_numerical.shape[1], dtype=torch.bool, device=x_numerical.device)\n",
    "\n",
    "        if pre_embedded_categorical is None:\n",
    "            embeds = [self.embedding_layers[key](x_categorical[key]) for key in self.categorical_keys] if self.categorical_keys else []\n",
    "        else:\n",
    "            embeds = pre_embedded_categorical\n",
    "\n",
    "        combined_features = torch.cat([x_numerical] + embeds, dim=2)\n",
    "        projected_features = self.input_projection(combined_features) * math.sqrt(self.d_model)\n",
    "        transformer_input = self.pos_encoder(projected_features)\n",
    "        \n",
    "        attention_weights = None\n",
    "        output = transformer_input\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            is_last_layer = (i == len(self.encoder_layers) - 1)\n",
    "            if is_last_layer and return_attention:\n",
    "                attn_output, attention_weights = layer.self_attn(\n",
    "                    output, output, output, \n",
    "                    key_padding_mask=src_key_padding_mask,\n",
    "                    need_weights=True,\n",
    "                    average_attn_weights=False\n",
    "                )\n",
    "                output = output + layer.dropout1(attn_output)\n",
    "                output = layer.norm1(output)\n",
    "                ff_output = layer.linear2(layer.dropout(layer.activation(layer.linear1(output))))\n",
    "                output = output + layer.dropout2(ff_output)\n",
    "                output = layer.norm2(output)\n",
    "            else:\n",
    "                output = layer(output, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        transformer_output = output\n",
    "        non_padding_mask = ~src_key_padding_mask\n",
    "        seq_lengths = non_padding_mask.sum(dim=1, keepdim=True)\n",
    "        masked_output = transformer_output * non_padding_mask.unsqueeze(-1)\n",
    "        summed_output = masked_output.sum(dim=1)\n",
    "        mean_output = summed_output / seq_lengths.clamp(min=1)\n",
    "        risk_score = self.fc(mean_output)\n",
    "        \n",
    "        if return_attention:\n",
    "            return risk_score, attention_weights\n",
    "        else:\n",
    "            return risk_score\n",
    "\n",
    "def cox_loss(risk_scores, times, events):\n",
    "    risk_scores = risk_scores.squeeze(-1)\n",
    "    sorted_indices = torch.argsort(times, descending=True)\n",
    "    risk_scores_sorted = risk_scores[sorted_indices]\n",
    "    events_sorted = events[sorted_indices]\n",
    "    log_risk_set_sum = torch.log(torch.cumsum(torch.exp(risk_scores_sorted), dim=0))\n",
    "    loss = -torch.sum(risk_scores_sorted[events_sorted.bool()] - log_risk_set_sum[events_sorted.bool()])\n",
    "    num_events = torch.sum(events)\n",
    "    if num_events > 0:\n",
    "        loss = loss / num_events\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, scaler, device):\n",
    "    model.train()\n",
    "    for batch_numerical_cpu, batch_categorical_cpu, (times_cpu, events_cpu) in dataloader:\n",
    "        batch_numerical = batch_numerical_cpu.to(device)\n",
    "        batch_categorical = {k: v.to(device) for k, v in batch_categorical_cpu.items()}\n",
    "        times, events = times_cpu.to(device), events_cpu.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=str(device).split(\":\")[0]):\n",
    "            risk_scores = model(batch_numerical, batch_categorical)\n",
    "            loss = loss_fn(risk_scores, times, events)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "def evaluate_model(model, dataloader, loss_fn, train_df_outcomes, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_risk_scores, all_times, all_events = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_numerical, batch_categorical, (times, events) in dataloader:\n",
    "            batch_numerical = batch_numerical.to(device)\n",
    "            batch_categorical = {k: v.to(device) for k, v in batch_categorical.items()}\n",
    "            with autocast(device_type=str(device).split(\":\")[0]):\n",
    "                risk_scores = model(batch_numerical, batch_categorical)\n",
    "                loss = loss_fn(risk_scores, times.to(device), events.to(device))\n",
    "            total_loss += loss.item()\n",
    "            all_risk_scores.append(risk_scores.cpu())\n",
    "            all_times.append(times.cpu())\n",
    "            all_events.append(events.cpu())\n",
    "\n",
    "    all_risk_scores_np = torch.cat(all_risk_scores).numpy()\n",
    "    all_times_np = torch.cat(all_times).numpy()\n",
    "    all_events_np = torch.cat(all_events).numpy()\n",
    "    \n",
    "    c_index = concordance_index(all_times_np, -all_risk_scores_np.squeeze(), all_events_np)\n",
    "    \n",
    "    train_outcomes_struct = np.array(list(zip(train_df_outcomes['dead'].astype(bool), train_df_outcomes['time'])), dtype=[('event', bool), ('time', float)])\n",
    "    val_outcomes_struct = np.array(list(zip(all_events_np.astype(bool), all_times_np)), dtype=[('event', bool), ('time', float)])\n",
    "    \n",
    "    event_times = train_df_outcomes[train_df_outcomes['dead'] == 1]['time']\n",
    "    if len(event_times) > 10:\n",
    "        min_time, max_time = np.quantile(event_times, [0.1, 0.9])\n",
    "        times_for_auc = np.linspace(min_time, max_time, 100) if max_time > min_time else [min_time]\n",
    "    else:\n",
    "        times_for_auc = np.quantile(train_df_outcomes['time'], [0.25, 0.5, 0.75])\n",
    "\n",
    "    try:\n",
    "        auc, mean_auc = cumulative_dynamic_auc(train_outcomes_struct, val_outcomes_struct, all_risk_scores_np.squeeze(), times_for_auc)\n",
    "        td_auc_df = pd.DataFrame({'time': times_for_auc, 'auc': auc})\n",
    "    except Exception as e:\n",
    "        print(f\"  - 警告: 计算 TD-AUC 失败。原因: {e}\")\n",
    "        mean_auc, td_auc_df = np.nan, None\n",
    "        \n",
    "    return total_loss / len(dataloader), c_index, mean_auc, td_auc_df\n",
    "\n",
    "def plot_fold_history(history, fold_num, output_dir=\"cv_plots\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 7))\n",
    "    \n",
    "    ax1.plot(epochs, history['train_loss'], 'bo-', label='Train Loss', markersize=3)\n",
    "    ax1.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss', markersize=3)\n",
    "    ax1.set_title(f'Fold {fold_num+1}: Loss Curve'); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Cox Loss'); ax1.legend(); ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, history['train_c_index'], 'bo-', label='Train C-Index', markersize=3)\n",
    "    ax2.plot(epochs, history['val_c_index'], 'ro-', label='Validation C-Index', markersize=3)\n",
    "    ax2.set_title(f'Fold {fold_num+1}: C-Index Curve'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('C-Index'); ax2.legend(); ax2.grid(True)\n",
    "    \n",
    "    ax3.plot(epochs, history['train_iauc'], 'bo-', label='Train iAUC', markersize=3)\n",
    "    ax3.plot(epochs, history['val_iauc'], 'ro-', label='Validation iAUC', markersize=3)\n",
    "    ax3.set_title(f'Fold {fold_num+1}: iAUC Curve'); ax3.set_xlabel('Epoch'); ax3.set_ylabel('iAUC'); ax3.legend(); ax3.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Fold {fold_num+1} Training & Validation History', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f\"fold_{fold_num+1}_training_history.png\")\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"图表已保存至: {save_path}\")\n",
    "\n",
    "def plot_average_risk_difference_histogram(all_results_dfs, set_name):\n",
    "    if not all_results_dfs:\n",
    "        print(\"警告: 没有可供汇总的风险差异结果。\")\n",
    "        return\n",
    "\n",
    "    # 1. 将所有折叠的DataFrame合并成一个\n",
    "    combined_results_df = pd.concat(all_results_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n--- 正在生成所有 {len(all_results_dfs)} 折的汇总风险差异直方图 ---\")\n",
    "    print(f\"  - 总共分析了 {len(combined_results_df)} 个患者-进展事件。\")\n",
    "\n",
    "    # 2. 绘图 (逻辑与单个直方图类似，但作用于汇总数据)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sns.histplot(data=combined_results_df, x='risk_difference', kde=False, bins=30, edgecolor=\"black\")\n",
    "    \n",
    "    median_risk_diff = combined_results_df['risk_difference'].median()\n",
    "    plt.axvline(median_risk_diff, color='red', linestyle='--', label=f'Overall Median Difference: {median_risk_diff:.4f}')\n",
    "    \n",
    "    plt.annotate(\n",
    "        'High-Impact (90th percentile)', \n",
    "        xy=(combined_results_df['risk_difference'].quantile(0.9), 5),\n",
    "        xytext=(combined_results_df['risk_difference'].quantile(0.9) + 1, 50),\n",
    "        arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "        fontsize=12\n",
    "    )\n",
    "\n",
    "    #plt.title(f\"Overall Distribution of Risk Differences ({set_name.upper()}, {len(all_results_dfs)}-Fold CV Average)\", fontsize=14)\n",
    "    plt.xlabel(\"Risk Difference (Factual Risk - Counterfactual Risk)\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Patients\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 3. 保存图像\n",
    "    output_dir = f\"./{set_name}/\"\n",
    "    save_path = os.path.join(output_dir, f\"average_progression_risk_difference_histogram.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"\\n汇总的风险差异分布图已保存至: {save_path}\")\n",
    "\n",
    "def visualize_attention_for_patient(\n",
    "    model, \n",
    "    patient_df, \n",
    "    numerical_cols, \n",
    "    categorical_cols, \n",
    "    sample_id, \n",
    "    fold_num, \n",
    "    death_status, \n",
    "    device,\n",
    "):\n",
    "    print(f\"  - 正在为样本 {sample_id} 生成注意力图...\")\n",
    "    model.to(device).eval()\n",
    "    categorical_cols_encoded = [c + '_encoded' for c in categorical_cols]\n",
    "    patient_dataset = PatientSequenceDataset(patient_df, numerical_cols, categorical_cols_encoded)\n",
    "    patient_loader = DataLoader(patient_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_pad)\n",
    "    batch_numerical, batch_categorical, _ = next(iter(patient_loader))\n",
    "    batch_numerical = batch_numerical.to(device)\n",
    "    batch_categorical = {k: v.to(device) for k, v in batch_categorical.items()}\n",
    "    with torch.no_grad():\n",
    "        _, attention_map = model(batch_numerical, batch_categorical, return_attention=True)\n",
    "    attention_map = attention_map.squeeze(0).cpu().mean(dim=0).numpy()\n",
    "    event_labels = [f\"T{i}: {st}\" for i, st in enumerate(patient_df['EVENT_SUBTYPE'])]\n",
    "    num_events = len(event_labels)\n",
    "    fig, ax = plt.subplots(figsize=(max(8, num_events/1.5), max(6, num_events/2)))\n",
    "    cax = ax.imshow(attention_map, cmap='viridis')\n",
    "    fig.colorbar(cax, label='Attention Weight')\n",
    "    ax.set_xticks(np.arange(num_events)); ax.set_yticks(np.arange(num_events))\n",
    "    ax.set_xticklabels(event_labels, rotation=90, ha=\"right\", fontsize=9)\n",
    "    ax.set_yticklabels(event_labels, fontsize=9)\n",
    "    ax.set_xlabel(\"Key (Attended To)\", fontsize=12); ax.set_ylabel(\"Query (Attending From)\", fontsize=12)\n",
    "    plt.title(f\"Attention Map for Sample {sample_id} (Status: {death_status}, Fold {fold_num})\", fontsize=14, pad=20)\n",
    "    plot_dir = f\"./{SET}/shap_plots_fold_{fold_num}\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    save_path = os.path.join(plot_dir, f\"attention_map_{sample_id}.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\"    - 注意力图已保存至: {save_path}\")\n",
    "\n",
    "def plot_shap_over_time(shap_values_tensor, input_tensor, sample_df, sample_id,\n",
    "                        feature_names, sample_ids_list, true_seq_lengths,\n",
    "                        fold_num, death_status, top_n_features=5):\n",
    "    print(f\"  - 正在为样本 {sample_id} 生成SHAP时序图...\")\n",
    "    try:\n",
    "        sample_idx = sample_ids_list.index(sample_id)\n",
    "    except (ValueError, IndexError):\n",
    "        print(f\"警告: 在SHAP分析的样本列表中未找到ID {sample_id}。\"); return\n",
    "    patient_shap = shap_values_tensor[sample_idx, :, :]\n",
    "    true_length = true_seq_lengths.get(sample_id, patient_shap.shape[0])\n",
    "    patient_shap = patient_shap[:true_length, :]\n",
    "    event_labels = [f\"T{i}: {st}\" for i, st in enumerate(sample_df[sample_df['SAMPLE_ID'] == sample_id]['EVENT_SUBTYPE'])]\n",
    "    total_importance = np.sum(np.abs(patient_shap), axis=0)\n",
    "    top_feature_indices = np.argsort(total_importance)[-top_n_features:]\n",
    "    plt.figure(figsize=(max(12, len(event_labels) * 0.8), 7))\n",
    "    colors = cm.get_cmap('tab10', len(top_feature_indices))\n",
    "    for i, f_idx in enumerate(top_feature_indices):\n",
    "        plt.plot(patient_shap[:, f_idx], marker='o', linestyle='-', label=feature_names[f_idx], color=colors(i))\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    plt.xticks(ticks=np.arange(len(event_labels)), labels=event_labels, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Event Sequence\"); plt.ylabel(\"SHAP Value (Impact on Risk)\")\n",
    "    plt.title(f\"Temporal SHAP Values for Sample {sample_id} (Status: {death_status}, Fold {fold_num})\")\n",
    "    plt.legend(title=\"Top Features\"); plt.grid(axis='x', linestyle=':', alpha=0.6); plt.tight_layout()\n",
    "    plot_dir = f\"./{SET}/shap_plots_fold_{fold_num}\"\n",
    "    save_path = os.path.join(plot_dir, f\"temporal_shap_{sample_id}.png\")\n",
    "    plt.savefig(save_path); plt.close()\n",
    "    print(f\"    - SHAP时序图已保存至: {save_path}\")\n",
    "\n",
    "def plot_waterfall(shap_values, feature_names, sample_id, title_suffix, save_dir, base_value, top_n=7):\n",
    "    explanation = shap.Explanation(\n",
    "        values=shap_values,\n",
    "        base_values=base_value,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    shap.plots.waterfall(explanation, max_display=top_n, show=False)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel(f\"Base Value = {base_value:.3f}\", fontsize=12)\n",
    "\n",
    "    #plt.title(f\"Aggregated SHAP Waterfall for Sample {sample_id}\\n({title_suffix})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    safe_suffix = title_suffix.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    fname = os.path.join(save_dir, f\"waterfall_{sample_id}_{safe_suffix}.png\")\n",
    "    plt.savefig(fname, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"    - 瀑布图已保存: {fname}\")\n",
    "\n",
    "def run_shap_analysis(model, train_df, val_df, numerical_cols, categorical_cols,\n",
    "                      fold_num, device='cuda', background_size=50, test_size=30):\n",
    "\n",
    "    print(f\"\\n--- [Fold {fold_num}] 开始SHAP可解释性分析 (device={device}) ---\")\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    print(\"  - 正在对验证集进行风险预测以筛选典型样本...\")\n",
    "    categorical_cols_encoded = [c + '_encoded' for c in categorical_cols]\n",
    "    val_dataset_full = PatientSequenceDataset(val_df, numerical_cols, categorical_cols_encoded)\n",
    "    if not val_dataset_full:\n",
    "        print(\"警告: 验证集为空，跳过SHAP分析。\")\n",
    "        return\n",
    "\n",
    "    val_loader_full = DataLoader(val_dataset_full, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad)\n",
    "\n",
    "    all_val_risks = []\n",
    "    with torch.no_grad():\n",
    "        for num, cat, _ in val_loader_full:\n",
    "            num = num.to(device)\n",
    "            cat_device = {k: v.to(device) for k, v in cat.items()}\n",
    "            risks = model(num, cat_device)\n",
    "            all_val_risks.extend(risks.squeeze().detach().cpu().numpy().flatten())\n",
    "\n",
    "    risk_df = pd.DataFrame({'SAMPLE_ID': val_dataset_full.sample_ids, 'predicted_risk': all_val_risks})\n",
    "    val_outcomes_df = val_df[['SAMPLE_ID', 'dead']].drop_duplicates()\n",
    "    risk_df = pd.merge(risk_df, val_outcomes_df, on='SAMPLE_ID')\n",
    "    average_risk = risk_df['predicted_risk'].mean()\n",
    "    print(f\"  - 验证集平均预测风险: {average_risk:.4f}\")\n",
    "\n",
    "    background_samples = list(train_df['SAMPLE_ID'].unique()[:background_size])\n",
    "    background_df = train_df[train_df['SAMPLE_ID'].isin(background_samples)].copy()\n",
    "    test_samples_for_shap = list(risk_df['SAMPLE_ID'].unique()[:test_size])\n",
    "    test_df_shap = val_df[val_df['SAMPLE_ID'].isin(test_samples_for_shap)].copy()\n",
    "\n",
    "    if not background_samples or not test_samples_for_shap:\n",
    "        print(\"警告: 背景或测试样本为空，跳过SHAP分析。\")\n",
    "        return\n",
    "\n",
    "    combined_df = pd.concat([background_df, test_df_shap])\n",
    "    combined_dataset = PatientSequenceDataset(combined_df, numerical_cols, categorical_cols_encoded)\n",
    "    if not combined_dataset:\n",
    "        print(\"警告: 组合数据集为空，跳过SHAP。\")\n",
    "        return\n",
    "\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=len(combined_dataset), shuffle=False, collate_fn=collate_fn_pad)\n",
    "    all_numericals_padded, all_categoricals_padded, _ = next(iter(combined_loader))\n",
    "\n",
    "    all_numericals_padded = all_numericals_padded.to(device)\n",
    "    all_categoricals_padded = {k: v.to(device) for k, v in all_categoricals_padded.items()}\n",
    "    combined_sample_order = combined_dataset.sample_ids\n",
    "\n",
    "    background_idx = [combined_sample_order.index(sid) for sid in background_samples if sid in combined_sample_order]\n",
    "    test_idx = [combined_sample_order.index(sid) for sid in test_samples_for_shap if sid in combined_sample_order]\n",
    "    if not background_idx or not test_idx:\n",
    "        print(\"警告: 批次中无背景或测试样本，跳过SHAP。\")\n",
    "        return\n",
    "\n",
    "    background_numericals_t = all_numericals_padded[background_idx]\n",
    "    test_numericals_t = all_numericals_padded[test_idx]\n",
    "    background_categoricals_t = {k: v[background_idx] for k, v in all_categoricals_padded.items()}\n",
    "    test_categoricals_t = {k: v[test_idx] for k, v in all_categoricals_padded.items()}\n",
    "\n",
    "    categorical_keys_sorted = model.categorical_keys\n",
    "    with torch.no_grad():\n",
    "        background_embeddings = [model.embedding_layers[key](background_categoricals_t[key]) for key in categorical_keys_sorted]\n",
    "        test_embeddings = [model.embedding_layers[key](test_categoricals_t[key]) for key in categorical_keys_sorted]\n",
    "\n",
    "    class SHAPWrapper(nn.Module):\n",
    "        def __init__(self, m): \n",
    "            super().__init__(); self.m = m\n",
    "        def forward(self, x_num, *pre_emb):\n",
    "            return self.m(x_num, pre_embedded_categorical=list(pre_emb))   \n",
    "\n",
    "    wrapper = SHAPWrapper(model)\n",
    "    background_inputs = [background_numericals_t.float()] + [be.float() for be in background_embeddings]\n",
    "    test_inputs = [test_numericals_t.float()] + [te.float() for te in test_embeddings]\n",
    "\n",
    "    explainer = shap.DeepExplainer(wrapper, background_inputs)\n",
    "\n",
    "    shap_values_list = explainer.shap_values(test_inputs, check_additivity=False)\n",
    "\n",
    "    def to_numpy(x):\n",
    "        if isinstance(x, list) or isinstance(x, tuple):\n",
    "            return [to_numpy(xx) for xx in x]\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return x\n",
    "        try:\n",
    "            if hasattr(x, 'detach'):\n",
    "                return x.detach().cpu().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return np.array(x)\n",
    "\n",
    "    shap_values_list = to_numpy(shap_values_list)\n",
    "\n",
    "    shap_numerical = shap_values_list[0]\n",
    "    if shap_numerical is None:\n",
    "        print(\"警告: 数值特征的 SHAP 值为空，跳过后续处理。\")\n",
    "        return\n",
    "\n",
    "    if shap_numerical.ndim == 4 and shap_numerical.shape[-1] == 1:\n",
    "        shap_numerical = np.squeeze(shap_numerical, axis=-1)  # 形状 -> (batch, seq, num_numerical_features)\n",
    "\n",
    "    shap_categorical_summed = []\n",
    "    for s in shap_values_list[1:]:\n",
    "        if s is None:\n",
    "            continue\n",
    "        if s.ndim == 4 and s.shape[-1] == 1:\n",
    "            s = np.squeeze(s, axis=-1)  # (batch, seq, embedding_dim)\n",
    "        summed_s = np.sum(s, axis=-1, keepdims=True)\n",
    "        shap_categorical_summed.append(summed_s)\n",
    "\n",
    "    all_shap_tensors = [shap_numerical] + shap_categorical_summed\n",
    "    try:\n",
    "        shap_values_temporal = np.concatenate(all_shap_tensors, axis=2)\n",
    "    except Exception as e:\n",
    "        print(\"错误: 在连接 SHAP 张量时发生异常：\", e)\n",
    "        print([arr.shape for arr in all_shap_tensors])\n",
    "        return\n",
    "\n",
    "    print(f\"  - SHAP值张量已成功构建，形状为: {shap_values_temporal.shape}\")\n",
    "\n",
    "    plot_dir = f\"./{SET}/shap_plots_fold_{fold_num}\"; os.makedirs(plot_dir, exist_ok=True)\n",
    "    feature_names = numerical_cols + categorical_keys_sorted\n",
    "    \n",
    "    shap_values_2d = shap_values_temporal.reshape(-1, shap_values_temporal.shape[-1])\n",
    "    input_df_for_shap = pd.DataFrame(shap_values_2d, columns=feature_names) \n",
    "\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values_2d, features=input_df_for_shap, feature_names=feature_names, show=False, max_display=15)\n",
    "    #plt.title(f\"Global Feature Importance (Fold {fold_num}) - Top 15\")\n",
    "    summary_path = os.path.join(plot_dir, \"summary_plot.png\"); plt.savefig(summary_path, bbox_inches='tight'); plt.close()\n",
    "    print(f\"  - 全局SHAP摘要图已保存: {summary_path}\")\n",
    "\n",
    "    sample_ids_in_shap_calc = [combined_sample_order[i] for i in test_idx]\n",
    "    risk_df_subset = risk_df[risk_df['SAMPLE_ID'].isin(sample_ids_in_shap_calc)]\n",
    "\n",
    "    high_risk_dead = risk_df_subset[(risk_df_subset['dead'] == 1) & (risk_df_subset['predicted_risk'] > average_risk)]['SAMPLE_ID'].tolist()\n",
    "    low_risk_alive = risk_df_subset[(risk_df_subset['dead'] == 0) & (risk_df_subset['predicted_risk'] < average_risk)]['SAMPLE_ID'].tolist()\n",
    "\n",
    "    print(f\"\\n  - 找到 {len(high_risk_dead)} 个典型死亡样本 (死亡且风险 > 平均值)。\")\n",
    "    print(f\"  - 找到 {len(low_risk_alive)} 个典型存活样本 (存活且风险 < 平均值)。\")\n",
    "\n",
    "    per_sample_feature_sum = np.sum(shap_values_temporal, axis=1)\n",
    "\n",
    "    ev = explainer.expected_value\n",
    "    try:\n",
    "        if hasattr(ev, 'item'):\n",
    "            base_value = ev.item()\n",
    "        elif isinstance(ev, (list, tuple, np.ndarray)):\n",
    "            base_value = float(ev[0])\n",
    "        else:\n",
    "            base_value = float(ev)\n",
    "    except Exception:\n",
    "        base_value = float(np.zeros(1))\n",
    "\n",
    "    if high_risk_dead:\n",
    "        sid = high_risk_dead[0]\n",
    "        risk_value = risk_df.loc[risk_df['SAMPLE_ID'] == sid, 'predicted_risk'].item()\n",
    "        print(f\"\\n  分析典型死亡样本: {sid} (预测风险: {risk_value:.4f})\")\n",
    "        sample_pos = sample_ids_in_shap_calc.index(sid)\n",
    "        plot_waterfall(per_sample_feature_sum[sample_pos], feature_names, sid, \"Dead (High Risk)\", plot_dir, base_value)\n",
    "\n",
    "    if low_risk_alive:\n",
    "        sid = low_risk_alive[0]\n",
    "        risk_value = risk_df.loc[risk_df['SAMPLE_ID'] == sid, 'predicted_risk'].item()\n",
    "        print(f\"\\n  分析典型存活样本: {sid} (预测风险: {risk_value:.4f})\")\n",
    "        sample_pos = sample_ids_in_shap_calc.index(sid)\n",
    "        plot_waterfall(per_sample_feature_sum[sample_pos], feature_names, sid, \"Alive (Low Risk)\", plot_dir, base_value)\n",
    "\n",
    "    print(f\"\\n--- [Fold {fold_num}] SHAP可解释性分析完成 ---\\n\")\n",
    "\n",
    "def explain_single_df(model, df_to_explain, background_df, scaler, vocabs, numerical_cols, categorical_cols, device):\n",
    "    \"\"\"\n",
    "    【修正版 v2】为单个任意DataFrame计算SHAP值，用于反事实解释。\n",
    "    修复了因特征列不匹配导致的scaler.transform错误。\n",
    "    \"\"\"\n",
    "    if df_to_explain.empty:\n",
    "        return None, None\n",
    "    \n",
    "    expected_numerical_features = list(scaler.feature_names_in_)\n",
    "\n",
    "    df_to_explain_norm = df_to_explain.copy()\n",
    "    for col in expected_numerical_features:\n",
    "        if col not in df_to_explain_norm.columns:\n",
    "            df_to_explain_norm[col] = 0.0\n",
    "    df_to_explain_norm.loc[:, expected_numerical_features] = scaler.transform(df_to_explain_norm[expected_numerical_features])\n",
    "\n",
    "    background_df_norm = background_df.copy()\n",
    "    for col in expected_numerical_features:\n",
    "        if col not in background_df_norm.columns:\n",
    "            background_df_norm[col] = 0.0\n",
    "    background_df_norm.loc[:, expected_numerical_features] = scaler.transform(background_df_norm[expected_numerical_features])\n",
    "    \n",
    "    categorical_cols_encoded = []\n",
    "    for col in categorical_cols:\n",
    "        if col in vocabs:\n",
    "            encoded_col_name = col + '_encoded'\n",
    "            categorical_cols_encoded.append(encoded_col_name)\n",
    "            df_to_explain_norm[encoded_col_name] = df_to_explain_norm[col].astype(str).map(vocabs[col]['vocab']).fillna(vocabs[col]['vocab']['<UNK>'])\n",
    "            background_df_norm[encoded_col_name] = background_df_norm[col].astype(str).map(vocabs[col]['vocab']).fillna(vocabs[col]['vocab']['<UNK>'])\n",
    "\n",
    "    combined_df = pd.concat([background_df_norm, df_to_explain_norm])\n",
    "    dataset = PatientSequenceDataset(combined_df, numerical_cols, categorical_cols_encoded)\n",
    "    if not dataset: return None, None\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, collate_fn=collate_fn_pad)\n",
    "    try:\n",
    "        numericals, categoricals, _ = next(iter(loader))\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "\n",
    "    model_cpu = model.to('cpu').eval()\n",
    "    bg_tensors = [numericals[:-1].cpu().float()] + [model_cpu.embedding_layers[k](categoricals[k][:-1].cpu()) for k in model_cpu.categorical_keys]\n",
    "    test_tensors = [numericals[-1:].cpu().float()] + [model_cpu.embedding_layers[k](categoricals[k][-1:].cpu()) for k in model_cpu.categorical_keys]\n",
    "    \n",
    "    class SHAPWrapper(nn.Module):\n",
    "        def __init__(self, m): super().__init__(); self.m = m\n",
    "        def forward(self, x_num, *pre_emb): return self.m(x_num, pre_embedded_categorical=list(pre_emb))\n",
    "\n",
    "    explainer = shap.DeepExplainer(SHAPWrapper(model_cpu), bg_tensors)\n",
    "    shap_values_list = explainer.shap_values(test_tensors, check_additivity=False)\n",
    "\n",
    "    shap_numerical = shap_values_list[0]\n",
    "    if shap_numerical.ndim == 4 and shap_numerical.shape[-1] == 1:\n",
    "        shap_numerical = np.squeeze(shap_numerical, axis=-1)\n",
    "    \n",
    "    shap_categorical_summed = []\n",
    "    for s in shap_values_list[1:]:\n",
    "        if s.ndim == 4 and s.shape[-1] == 1: s = np.squeeze(s, axis=-1)\n",
    "        summed_s = np.sum(s, axis=-1, keepdims=True)\n",
    "        shap_categorical_summed.append(summed_s)\n",
    "        \n",
    "    shap_temporal = np.concatenate([shap_numerical] + shap_categorical_summed, axis=2)\n",
    "    \n",
    "    aggregated_shap_values = np.sum(shap_temporal, axis=1).squeeze(0)\n",
    "    base_value = explainer.expected_value[0].item() if hasattr(explainer.expected_value, 'item') else explainer.expected_value[0]\n",
    "    \n",
    "    return aggregated_shap_values, base_value\n",
    "\n",
    "def _get_prediction_for_processed_df(df_processed, model, numerical_cols, categorical_cols_encoded, device):\n",
    "    if df_processed.empty:\n",
    "        return np.nan\n",
    "        \n",
    "    temp_dataset = PatientSequenceDataset(df_processed, numerical_cols, categorical_cols_encoded)\n",
    "    if len(temp_dataset) == 0: return np.nan\n",
    "    \n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_pad)\n",
    "    \n",
    "    try:\n",
    "        batch_numerical, batch_categorical, _ = next(iter(temp_loader))\n",
    "    except StopIteration:\n",
    "        return np.nan\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_numerical = batch_numerical.to(device)\n",
    "    batch_categorical = {k: v.to(device) for k, v in batch_categorical.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        risk_score = model(batch_numerical, batch_categorical)\n",
    "        \n",
    "    return risk_score.squeeze().cpu().item()\n",
    "\n",
    "def _create_predictor_from_raw(model, numerical_cols, categorical_cols, scaler, vocabs, device):\n",
    "    def predictor(raw_df):\n",
    "        if raw_df.empty:\n",
    "            return np.nan\n",
    "        \n",
    "        df_processed = raw_df.copy()\n",
    "        \n",
    "        expected_numerical_features = list(scaler.feature_names_in_)\n",
    "\n",
    "        for col in expected_numerical_features:\n",
    "            if col not in df_processed.columns:\n",
    "                df_processed[col] = 0.0\n",
    "\n",
    "        df_to_scale = df_processed[expected_numerical_features]\n",
    "\n",
    "        scaled_data = scaler.transform(df_to_scale)\n",
    "        df_processed.loc[:, expected_numerical_features] = scaled_data\n",
    "\n",
    "        categorical_cols_encoded = []\n",
    "        for col in categorical_cols:\n",
    "            if col in vocabs:\n",
    "                encoded_col_name = col + '_encoded'\n",
    "                categorical_cols_encoded.append(encoded_col_name)\n",
    "                df_processed[encoded_col_name] = df_processed[col].astype(str).map(vocabs[col]['vocab'])\n",
    "                df_processed[encoded_col_name].fillna(vocabs[col]['vocab']['<UNK>'], inplace=True)\n",
    "\n",
    "        return _get_prediction_for_processed_df(\n",
    "            df_processed, model, numerical_cols, categorical_cols_encoded, device\n",
    "        )\n",
    "    return predictor\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def analyze_progression_impact_heterogeneity(results_df, val_df, static_features_list, fold_num):\n",
    "    print(f\"\\n--- [Fold {fold_num}] 开始生成进展影响异质性总结表 ---\")\n",
    "    \n",
    "    if results_df is None or len(results_df) < 10:\n",
    "        print(\"  - 结果太少，无法进行有意义的异质性分析。跳过。\")\n",
    "        return\n",
    "\n",
    "    quantiles = results_df['risk_difference'].quantile([0.25, 0.75])\n",
    "    low_impact_threshold, high_impact_threshold = quantiles[0.25], quantiles[0.75]\n",
    "    results_df['impact_group'] = pd.cut(\n",
    "        results_df['risk_difference'],\n",
    "        bins=[-np.inf, low_impact_threshold, high_impact_threshold, np.inf],\n",
    "        labels=['Low-Impact', 'Mid-Impact', 'High-Impact']\n",
    "    )\n",
    "    analysis_df = results_df[results_df['impact_group'].isin(['Low-Impact', 'High-Impact'])].copy()\n",
    "    if analysis_df.empty or analysis_df['impact_group'].nunique() < 2:\n",
    "        print(\"  - 未能成功划分出高/低影响组。跳过。\"); return\n",
    "\n",
    "    static_features_df = val_df.sort_values('START_DATE').drop_duplicates(subset='PATIENT_ID', keep='first')\n",
    "    analysis_df = pd.merge(analysis_df, static_features_df, on='PATIENT_ID', how='left')\n",
    "\n",
    "    table_data = []\n",
    "    group_low = analysis_df[analysis_df['impact_group'] == 'Low-Impact']\n",
    "    group_high = analysis_df[analysis_df['impact_group'] == 'High-Impact']\n",
    "    n_low, n_high = len(group_low), len(group_high)\n",
    "    print(f\"  - 低影响组 N={n_low}, 高影响组 N={n_high}\")\n",
    "\n",
    "    features_to_compare = [f for f in static_features_list if f in analysis_df.columns]\n",
    "    \n",
    "    for feature in features_to_compare:\n",
    "        if analysis_df[feature].nunique() < 2: continue\n",
    "\n",
    "        stat_row = {'Baseline Characteristic': feature}\n",
    "        \n",
    "        is_continuous = pd.api.types.is_float_dtype(analysis_df[feature]) or \\\n",
    "                        (pd.api.types.is_integer_dtype(analysis_df[feature]) and analysis_df[feature].nunique() > 2)\n",
    "\n",
    "        if is_continuous:\n",
    "            median_low, (q1_low, q3_low) = group_low[feature].median(), group_low[feature].quantile([0.25, 0.75])\n",
    "            stat_row[f'Low-Impact Group (N={n_low})'] = f\"{median_low:.0f} [{q1_low:.0f}-{q3_low:.0f}]\"\n",
    "            median_high, (q1_high, q3_high) = group_high[feature].median(), group_high[feature].quantile([0.25, 0.75])\n",
    "            stat_row[f'High-Impact Group (N={n_high})'] = f\"{median_high:.0f} [{q1_high:.0f}-{q3_high:.0f}]\"\n",
    "            _, p_value = ttest_ind(group_low[feature].dropna(), group_high[feature].dropna(), equal_var=False)\n",
    "        else:\n",
    "            count_low, perc_low = group_low[feature].sum(), (group_low[feature].sum() / n_low * 100) if n_low > 0 else 0\n",
    "            stat_row[f'Low-Impact Group (N={n_low})'] = f\"{count_low} ({perc_low:.1f}%)\"\n",
    "            count_high, perc_high = group_high[feature].sum(), (group_high[feature].sum() / n_high * 100) if n_high > 0 else 0\n",
    "            stat_row[f'High-Impact Group (N={n_high})'] = f\"{count_high} ({perc_high:.1f}%)\"\n",
    "            contingency_table = pd.crosstab(analysis_df['impact_group'], analysis_df[feature])\n",
    "            _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "        stat_row['p-value'] = f\"<0.001\" if p_value < 0.001 else f\"{p_value:.3f}\"\n",
    "        table_data.append(stat_row)\n",
    "\n",
    "    summary_table = pd.DataFrame(table_data)\n",
    "    output_dir = f\"./{SET}/impact_heterogeneity_tables_fold_{fold_num}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_path = os.path.join(output_dir, \"table2_impact_group_comparison.csv\")\n",
    "    summary_table.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(\"\\n--- 异质性分析总结表 (Table 2) ---\")\n",
    "    print(summary_table.to_string(index=False))\n",
    "    print(f\"\\n  表格已保存至: {save_path}\")\n",
    "\n",
    "def summarize_heterogeneity_across_folds(analysis_data_list, static_features_list, set_name):\n",
    "    print(f\"\\n--- 正在汇总所有 {len(analysis_data_list)} 折的进展影响异质性分析 ---\")\n",
    "    \n",
    "    if not analysis_data_list:\n",
    "        print(\"  - 没有可供汇总的异质性分析数据。\")\n",
    "        return\n",
    "\n",
    "    all_results = []\n",
    "    for data in analysis_data_list:\n",
    "        results_df = data['results_df']\n",
    "        val_df = data['val_df']\n",
    "        \n",
    "        static_features_df = val_df.sort_values('START_DATE').drop_duplicates(subset='PATIENT_ID', keep='first')\n",
    "        merged_df = pd.merge(results_df, static_features_df, on='PATIENT_ID', how='left')\n",
    "        all_results.append(merged_df)\n",
    "\n",
    "    combined_analysis_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    quantiles = combined_analysis_df['risk_difference'].quantile([0.25, 0.75])\n",
    "    low_impact_threshold = quantiles[0.25]\n",
    "    high_impact_threshold = quantiles[0.75]\n",
    "\n",
    "    combined_analysis_df['impact_group'] = pd.cut(\n",
    "        combined_analysis_df['risk_difference'],\n",
    "        bins=[-np.inf, low_impact_threshold, high_impact_threshold, np.inf],\n",
    "        labels=['Low-Impact', 'Mid-Impact', 'High-Impact']\n",
    "    )\n",
    "    \n",
    "    analysis_df = combined_analysis_df[combined_analysis_df['impact_group'].isin(['Low-Impact', 'High-Impact'])].copy()\n",
    "    \n",
    "    if analysis_df.empty or analysis_df['impact_group'].nunique() < 2:\n",
    "        print(\"  - 未能从汇总数据中成功划分出高/低影响组。跳过。\")\n",
    "        return\n",
    "    \n",
    "    table_data = []\n",
    "    group_low = analysis_df[analysis_df['impact_group'] == 'Low-Impact']\n",
    "    group_high = analysis_df[analysis_df['impact_group'] == 'High-Impact']\n",
    "    n_low, n_high = len(group_low), len(group_high)\n",
    "\n",
    "    print(f\"  - 汇总分析: 低影响组 N={n_low}, 高影响组 N={n_high}\")\n",
    "\n",
    "    features_to_compare = [f for f in static_features_list if f in analysis_df.columns]\n",
    "    \n",
    "    for feature in features_to_compare:\n",
    "        if analysis_df[feature].nunique() < 2: continue\n",
    "\n",
    "        stat_row = {'Baseline Characteristic': feature}\n",
    "        \n",
    "        is_continuous = pd.api.types.is_float_dtype(analysis_df[feature]) or \\\n",
    "                        (pd.api.types.is_integer_dtype(analysis_df[feature]) and analysis_df[feature].nunique() > 2)\n",
    "\n",
    "        if is_continuous:\n",
    "            median_low, (q1_low, q3_low) = group_low[feature].median(), group_low[feature].quantile([0.25, 0.75])\n",
    "            stat_row[f'Low-Impact Group (N={n_low})'] = f\"{median_low:.0f} [{q1_low:.0f}-{q3_low:.0f}]\"\n",
    "            median_high, (q1_high, q3_high) = group_high[feature].median(), group_high[feature].quantile([0.25, 0.75])\n",
    "            stat_row[f'High-Impact Group (N={n_high})'] = f\"{median_high:.0f} [{q1_high:.0f}-{q3_high:.0f}]\"\n",
    "            _, p_value = ttest_ind(group_low[feature].dropna(), group_high[feature].dropna(), equal_var=False)\n",
    "        else:\n",
    "            count_low, perc_low = group_low[feature].sum(), (group_low[feature].sum() / n_low * 100) if n_low > 0 else 0\n",
    "            stat_row[f'Low-Impact Group (N={n_low})'] = f\"{int(count_low)} ({perc_low:.1f}%)\"\n",
    "            count_high, perc_high = group_high[feature].sum(), (group_high[feature].sum() / n_high * 100) if n_high > 0 else 0\n",
    "            stat_row[f'High-Impact Group (N={n_high})'] = f\"{int(count_high)} ({perc_high:.1f}%)\"\n",
    "            contingency_table = pd.crosstab(analysis_df['impact_group'], analysis_df[feature])\n",
    "            _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "        stat_row['p-value'] = f\"<0.001\" if p_value < 0.001 else f\"{p_value:.3f}\"\n",
    "        table_data.append(stat_row)\n",
    "\n",
    "    summary_table = pd.DataFrame(table_data)\n",
    "    \n",
    "    output_dir = f\"./{set_name}/\"\n",
    "    save_path = os.path.join(output_dir, f\"summary_table_impact_group_comparison_{set_name}.csv\")\n",
    "    \n",
    "    summary_table.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(\"\\n--- 汇总的异质性分析总结表 (Table 2) ---\")\n",
    "    print(summary_table.to_string(index=False))\n",
    "    print(f\"\\n  汇总表格已保存至: {save_path}\")\n",
    "\n",
    "def explain_counterfactual_pair(model, factual_df, counterfactual_df, background_df,\n",
    "                                scaler, vocabs, numerical_cols, categorical_cols, device):\n",
    "    if factual_df.empty or counterfactual_df.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    expected_numerical_features = list(scaler.feature_names_in_)\n",
    "\n",
    "    dataframes_to_process = {\n",
    "        \"background\": background_df,\n",
    "        \"factual\": factual_df,\n",
    "        \"counterfactual\": counterfactual_df\n",
    "    }\n",
    "    processed_dfs = {}\n",
    "\n",
    "    for name, df in dataframes_to_process.items():\n",
    "        df_norm = df.copy()\n",
    "        for col in expected_numerical_features:\n",
    "            if col not in df_norm.columns:\n",
    "                df_norm[col] = 0.0\n",
    "        df_norm.loc[:, expected_numerical_features] = scaler.transform(df_norm[expected_numerical_features])\n",
    "        processed_dfs[name] = df_norm\n",
    "    \n",
    "    background_df_norm = processed_dfs[\"background\"]\n",
    "    factual_df_norm = processed_dfs[\"factual\"]\n",
    "    cf_df_norm = processed_dfs[\"counterfactual\"]\n",
    "\n",
    "    categorical_cols_encoded = []\n",
    "    for col in categorical_cols:\n",
    "        if col in vocabs:\n",
    "            encoded_col_name = col + '_encoded'\n",
    "            categorical_cols_encoded.append(encoded_col_name)\n",
    "            for df in [background_df_norm, factual_df_norm, cf_df_norm]:\n",
    "                df[encoded_col_name] = df[col].astype(str).map(vocabs[col]['vocab']).fillna(vocabs[col]['vocab']['<UNK>'])\n",
    "\n",
    "    combined_df = pd.concat([background_df_norm, factual_df_norm, cf_df_norm])\n",
    "    dataset = PatientSequenceDataset(combined_df, numerical_cols, categorical_cols_encoded)\n",
    "    if not dataset: return None, None, None\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, collate_fn=collate_fn_pad)\n",
    "    try:\n",
    "        numericals, categoricals, _ = next(iter(loader))\n",
    "    except StopIteration:\n",
    "        return None, None, None\n",
    "\n",
    "    model_cpu = model.to('cpu').eval()\n",
    "    bg_tensors = [numericals[:-2].cpu().float()] + [model_cpu.embedding_layers[k](categoricals[k][:-2].cpu()) for k in model_cpu.categorical_keys]\n",
    "    test_tensors = [numericals[-2:].cpu().float()] + [model_cpu.embedding_layers[k](categoricals[k][-2:].cpu()) for k in model_cpu.categorical_keys]\n",
    "    \n",
    "    class SHAPWrapper(nn.Module):\n",
    "        def __init__(self, m): super().__init__(); self.m = m\n",
    "        def forward(self, x_num, *pre_emb): return self.m(x_num, pre_embedded_categorical=list(pre_emb))\n",
    "\n",
    "    explainer = shap.DeepExplainer(SHAPWrapper(model_cpu), bg_tensors)\n",
    "    shap_values_list = explainer.shap_values(test_tensors, check_additivity=False)\n",
    "\n",
    "    shap_numerical = shap_values_list[0]\n",
    "    if shap_numerical.ndim == 4 and shap_numerical.shape[-1] == 1:\n",
    "        shap_numerical = np.squeeze(shap_numerical, axis=-1)\n",
    "    \n",
    "    shap_categorical_summed = []\n",
    "    for s in shap_values_list[1:]:\n",
    "        if s.ndim == 4 and s.shape[-1] == 1: s = np.squeeze(s, axis=-1)\n",
    "        summed_s = np.sum(s, axis=-1, keepdims=True)\n",
    "        shap_categorical_summed.append(summed_s)\n",
    "        \n",
    "    shap_temporal = np.concatenate([shap_numerical] + shap_categorical_summed, axis=2)\n",
    "    aggregated_shap_values = np.sum(shap_temporal, axis=1)\n",
    "    \n",
    "    factual_shap = aggregated_shap_values[0]\n",
    "    counterfactual_shap = aggregated_shap_values[1]\n",
    "    base_value = explainer.expected_value[0].item() if hasattr(explainer.expected_value, 'item') else explainer.expected_value[0]\n",
    "    \n",
    "    return factual_shap, counterfactual_shap, base_value\n",
    "\n",
    "def run_progression_counterfactual(model, train_df_norm, val_df, final_first_progression_df, progression_event_def, \n",
    "                                 counterfactual_change, scaler, vocabs, numerical_cols, \n",
    "                                 categorical_cols, fold_num, device='cpu', n=100):\n",
    "    print(f\"\\n--- [Fold {fold_num}] 开始疾病进展反事实模拟与解释 ---\")\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    predictor = _create_predictor_from_raw(model, numerical_cols, categorical_cols, scaler, vocabs, device)\n",
    "    \n",
    "    pids_in_val = val_df['PATIENT_ID'].unique()\n",
    "    target_progression_df = final_first_progression_df[final_first_progression_df['PATIENT_ID'].isin(pids_in_val)]\n",
    "    if target_progression_df.empty:\n",
    "        print(\"  - 当前验证集中没有找到指定的进展患者。跳过。\"); return None\n",
    "    \n",
    "    results, visualized_one = [], False\n",
    "    for _, row in target_progression_df.iterrows():\n",
    "        patient_id = row['PATIENT_ID']\n",
    "        patient_df = val_df[val_df['PATIENT_ID'] == patient_id].copy()\n",
    "        \n",
    "        factual_df = patient_df[patient_df['START_DATE'] <= row['START_DATE']]\n",
    "        history_before = patient_df[patient_df['START_DATE'] < row['START_DATE']]\n",
    "        prog_event_mask = (patient_df['START_DATE'] == row['START_DATE']) & (patient_df['EVENT_SUBTYPE'] == progression_event_def['EVENT_SUBTYPE'])\n",
    "        prog_event = patient_df[prog_event_mask]\n",
    "        if prog_event.empty: continue\n",
    "        \n",
    "        cf_event = prog_event.iloc[0:1].copy()\n",
    "        for col, val in counterfactual_change.items(): cf_event[col] = val\n",
    "        counterfactual_df = pd.concat([history_before, cf_event], ignore_index=True)\n",
    "        \n",
    "        factual_risk = predictor(factual_df); counterfactual_risk = predictor(counterfactual_df)\n",
    "        if not np.isnan(factual_risk) and not np.isnan(counterfactual_risk):\n",
    "            results.append({'PATIENT_ID': patient_id, 'factual_risk': factual_risk, 'counterfactual_risk': counterfactual_risk, 'risk_difference': factual_risk - counterfactual_risk})\n",
    "\n",
    "        if not visualized_one:\n",
    "            print(f\"  - 为患者 {patient_id} 生成反事实SHAP瀑布图...\")\n",
    "            feature_names = numerical_cols + sorted([c for c in categorical_cols if c in vocabs])\n",
    "            plot_dir = f\"./{SET}/counterfactual_plots_fold_{fold_num}\"\n",
    "            os.makedirs(plot_dir, exist_ok=True)\n",
    "            background_for_cf = train_df_norm.sample(n=n, random_state=SEED)\n",
    "\n",
    "            factual_shap, cf_shap, base_val = explain_counterfactual_pair(\n",
    "                model, factual_df, counterfactual_df, background_for_cf,\n",
    "                scaler, vocabs, numerical_cols, categorical_cols, device\n",
    "            )\n",
    "\n",
    "            if factual_shap is not None and cf_shap is not None:\n",
    "                title_suffix_factual = f\"Factual (Risk_ {factual_risk:.3f})\"\n",
    "                plot_waterfall(factual_shap, feature_names, patient_id, title_suffix_factual, plot_dir, base_val)\n",
    "                \n",
    "                title_suffix_cf = f\"Counterfactual (Risk_ {counterfactual_risk:.3f})\"\n",
    "                plot_waterfall(cf_shap, feature_names, patient_id, title_suffix_cf, plot_dir, base_val)\n",
    "            \n",
    "            visualized_one = True\n",
    "    \n",
    "    if not results:\n",
    "        print(\"  - 未能成功为任何进展患者生成模拟结果。\"); return None\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    avg_risk_diff = results_df['risk_difference'].mean()\n",
    "    print(f\"  - 平均风险差异 (真实进展 vs 虚拟稳定): {avg_risk_diff:.4f}\")\n",
    "\n",
    "    # (绘制直方图的代码保持不变)\n",
    "    plt.figure(figsize=(10, 6)); sns.histplot(data=results_df, x='risk_difference', kde=False, bins=25, edgecolor=\"black\")\n",
    "    median_risk_diff = results_df['risk_difference'].median()\n",
    "    plt.axvline(median_risk_diff, color='red', linestyle='--', label=f'Median Difference: {median_risk_diff:.4f}')\n",
    "    plt.title(f\"Distribution of Risk Differences (Fold {fold_num})\\nProgression vs. No Progression\")\n",
    "    plt.xlabel(\"Risk Difference (Factual Risk - Counterfactual Risk)\"); plt.ylabel(\"Number of Patients\"); plt.legend()\n",
    "    save_path = os.path.join(plot_dir, \"progression_risk_difference_histogram.png\"); plt.savefig(save_path); plt.close()\n",
    "    print(f\"\\n风险差异分布图已保存至: {save_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Block (MODIFIED FOR TRAIN/ANALYZE MODES)\n",
    "# =============================================================================\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODE = 'analyze' # 'train' 或 'analyze'\n",
    "\n",
    "SET = 'brca'\n",
    "data_filename = f'df_{SET}_landmarks.csv'\n",
    "print(f\"\\n--- 当前模式: {MODE.upper()} | 数据集: {SET.upper()} ---\")\n",
    "\n",
    "set_features = {\n",
    "    'brca': ['AGE', 'STAGE 1', 'STAGE 3', 'STAGE 4', 'PTEN', 'ERBB2', 'TP53'],\n",
    "    'crc': ['AGE', 'BLACK', 'STAGE 1', 'STAGE 2', 'STAGE 4', 'KRAS', 'BRAF'],\n",
    "    'nsclc': ['AGE', 'MALE', 'STAFE 1', 'STAGE 3', 'STAGE 4', 'PTEN', 'EGFR', 'TP53'],\n",
    "    'panc': ['AGE', 'MALE', 'STAGE 1', 'STAGE 4', 'KRAS', 'TP53'],\n",
    "    'prostate': ['AGE', 'BLACK', 'STAGE 4', 'PTEN', 'TP53']\n",
    "}\n",
    "CORE_FEATURES = ['START_DATE', 'VALUE_NUMERIC', 'EVENT_DURATION', 'EVENT_TYPE', 'EVENT_SUBTYPE', 'VALUE_CATEGORICAL']\n",
    "ALL_FEATURES = CORE_FEATURES + set_features[SET]\n",
    "STANDARDIZE_COLS = ['START_DATE', 'EVENT_DURATION', 'VALUE_NUMERIC', 'AGE']\n",
    "CATEGORICAL_COLS = ['EVENT_TYPE', 'EVENT_SUBTYPE', 'VALUE_CATEGORICAL']\n",
    "\n",
    "model_params = {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 1024, 'dropout_prob': 0.45}\n",
    "optimizer_params = {'lr': 8.1e-06, 'weight_decay': 0.00037}\n",
    "N_SPLITS = 5\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "data_filename = f'df_{SET}_landmarks.csv'\n",
    "\n",
    "if not os.path.exists(data_filename):\n",
    "    print(f\"错误: 未找到数据文件 '{data_filename}'。\")\n",
    "else:\n",
    "    time_start = time.time()\n",
    "    df_full = pd.read_csv(data_filename)\n",
    "    if 'time' not in df_full.columns: df_full['time'] = df_full['stop']\n",
    "    df_processed = preprocess_dataframe(df_full)\n",
    "    \n",
    "    existing_features = [f for f in ALL_FEATURES if f in df_processed.columns]\n",
    "    final_categorical_cols = [c for c in CATEGORICAL_COLS if c in existing_features]\n",
    "    final_numerical_cols = sorted(list(set(existing_features) - set(final_categorical_cols)))\n",
    "    final_standardize_cols = [c for c in STANDARDIZE_COLS if c in final_numerical_cols]\n",
    "\n",
    "    print(f\"\\n模型将使用以下特征:\\n  - 数值: {final_numerical_cols}\\n  - 分类: {final_categorical_cols}\")\n",
    "\n",
    "    patient_outcomes = df_processed[['PATIENT_ID', 'dead']].drop_duplicates()\n",
    "    patient_ids = patient_outcomes['PATIENT_ID'].values\n",
    "    patient_dead_status = patient_outcomes['dead'].values\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # =========================================================================\n",
    "    #  TRAINING MODE\n",
    "    # =========================================================================\n",
    "    if MODE == 'train':\n",
    "        print(\"\\n--- 开始训练模式 ---\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(patient_ids, patient_dead_status)):\n",
    "            print(f\"\\n{'='*80}\\n===============  开始训练第 {fold+1}/{N_SPLITS} 折  ===============\\n{'='*80}\")\n",
    "\n",
    "            train_patient_ids, val_patient_ids = patient_ids[train_idx], patient_ids[val_idx]\n",
    "            train_df = df_processed[df_processed['PATIENT_ID'].isin(train_patient_ids)].copy()\n",
    "            val_df = df_processed[df_processed['PATIENT_ID'].isin(val_patient_ids)].copy()\n",
    "            \n",
    "            train_df_encoded, val_df_encoded, vocabs = encode_categorical_features_leakproof(train_df, val_df, final_categorical_cols)\n",
    "            train_df_norm, val_df_norm, feature_scaler = normalize_numerical_features_leakproof(train_df_encoded, val_df_encoded, final_standardize_cols)\n",
    "\n",
    "            categorical_cols_encoded = [c + '_encoded' for c in final_categorical_cols]\n",
    "            train_dataset = PatientSequenceDataset(train_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "            val_dataset = PatientSequenceDataset(val_df_norm, final_numerical_cols, categorical_cols_encoded)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_pad)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad)\n",
    "            train_outcomes_df = train_df[['PATIENT_ID', 'dead', 'time']].drop_duplicates()\n",
    "            \n",
    "            vocab_sizes = {k: v['vocab_size'] for k, v in vocabs.items()}\n",
    "            embedding_dims = {k: min(50, (v['vocab_size'] // 2) + 1) for k, v in vocabs.items()}\n",
    "            model = SurvivalTransformer(vocab_sizes, embedding_dims, len(final_numerical_cols), **model_params).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), **optimizer_params, fused=torch.cuda.is_available())\n",
    "            grad_scaler = GradScaler()\n",
    "            \n",
    "            best_val_iauc = -1\n",
    "\n",
    "            output_dir = f\"./{SET}/fold_{fold+1}/\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            best_model_path = os.path.join(output_dir, f\"best_model_fold_{fold+1}.pth\")\n",
    "            \n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                train_one_epoch(model, train_loader, optimizer, cox_loss, grad_scaler, DEVICE)\n",
    "                _, _, val_iauc, _ = evaluate_model(model, val_loader, cox_loss, train_outcomes_df, DEVICE)\n",
    "                if val_iauc > best_val_iauc:\n",
    "                    best_val_iauc = val_iauc\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                if (epoch + 1) % 20 == 0:\n",
    "                    print(f\"  Epoch {epoch+1:03d}/{NUM_EPOCHS} | Val iAUC: {val_iauc if not np.isnan(val_iauc) else 0:.4f} (Best: {best_val_iauc:.4f})\")\n",
    "\n",
    "            print(f\"--- 第 {fold+1} 折训练完成。最佳验证iAUC: {best_val_iauc:.4f} ---\")\n",
    "            print(f\"  - 模型已保存至: {best_model_path}\")\n",
    "\n",
    "            with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as f:\n",
    "                pickle.dump(feature_scaler, f)\n",
    "            with open(os.path.join(output_dir, 'vocabs.pkl'), 'wb') as f:\n",
    "                pickle.dump(vocabs, f)\n",
    "            print(f\"  - Scaler 和 Vocabs 已保存至: {output_dir}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    #  ANALYSIS MODE\n",
    "    # =========================================================================\n",
    "    elif MODE == 'analyze':\n",
    "        print(\"\\n--- 开始分析模式 ---\")\n",
    "        all_folds_td_auc = []\n",
    "        final_fold_metrics = []\n",
    "        all_progression_results_dfs = [] \n",
    "        heterogeneity_analysis_data = []\n",
    "        \n",
    "        for fold in range(N_SPLITS):\n",
    "            print(f\"\\n{'='*80}\\n===============  开始分析第 {fold+1}/{N_SPLITS} 折  ===============\\n{'='*80}\")\n",
    "\n",
    "            input_dir = f\"./{SET}/fold_{fold+1}/\"\n",
    "            model_path = os.path.join(input_dir, f\"best_model_fold_{fold+1}.pth\")\n",
    "            scaler_path = os.path.join(input_dir, 'scaler.pkl')\n",
    "            vocabs_path = os.path.join(input_dir, 'vocabs.pkl')\n",
    "            \n",
    "            if not all(os.path.exists(p) for p in [model_path, scaler_path, vocabs_path]):\n",
    "                print(f\"错误: 在 '{input_dir}' 中缺少必要的模型或预处理文件。请先运行 'train' 模式。\")\n",
    "                continue\n",
    "\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                feature_scaler = pickle.load(f)\n",
    "            with open(vocabs_path, 'rb') as f:\n",
    "                vocabs = pickle.load(f)\n",
    "            print(f\"  - 已从 '{input_dir}' 加载 Scaler 和 Vocabs。\")\n",
    "\n",
    "            train_idx, val_idx = list(kf.split(patient_ids, patient_dead_status))[fold]\n",
    "            train_patient_ids, val_patient_ids = patient_ids[train_idx], patient_ids[val_idx]\n",
    "            \n",
    "            train_df = df_processed[df_processed['PATIENT_ID'].isin(train_patient_ids)].copy()\n",
    "            val_df = df_processed[df_processed['PATIENT_ID'].isin(val_patient_ids)].copy()\n",
    "\n",
    "            train_df_encoded, val_df_encoded, _ = encode_categorical_features_leakproof(train_df, val_df, final_categorical_cols)\n",
    "            train_df_norm, val_df_norm, _ = normalize_numerical_features_leakproof(train_df_encoded, val_df_encoded, final_standardize_cols)\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                PatientSequenceDataset(val_df_norm, final_numerical_cols, [c+'_encoded' for c in final_categorical_cols]),\n",
    "                batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_pad\n",
    "            )\n",
    "            train_outcomes_df = train_df[['PATIENT_ID', 'dead', 'time']].drop_duplicates()\n",
    "\n",
    "            vocab_sizes = {k: v['vocab_size'] for k, v in vocabs.items()}\n",
    "            embedding_dims = {k: min(50, (v['vocab_size'] // 2) + 1) for k, v in vocabs.items()}\n",
    "            \n",
    "            best_model = SurvivalTransformer(vocab_sizes, embedding_dims, len(final_numerical_cols), **model_params)\n",
    "            best_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "            best_model.to(DEVICE).eval()\n",
    "            print(f\"  - 已从 '{model_path}' 加载最佳模型。\")\n",
    "\n",
    "            final_loss, final_c, final_iauc, final_td_auc_df = evaluate_model(best_model, val_loader, cox_loss, train_outcomes_df, DEVICE)\n",
    "            final_fold_metrics.append({'fold': fold + 1, 'val_loss': final_loss, 'val_c_index': final_c, 'val_iauc': final_iauc})\n",
    "            if final_td_auc_df is not None:\n",
    "                all_folds_td_auc.append(final_td_auc_df)\n",
    "                print(f\"\\n  [Fold {fold+1}] 最终验证集TD-AUC详情:\\n{final_td_auc_df.to_string(index=False, float_format='%.4f')}\")\n",
    "\n",
    "            time_1 = time.time()\n",
    "            run_shap_analysis(model=best_model, train_df=train_df_norm, val_df=val_df_norm, numerical_cols=final_numerical_cols, \n",
    "                              categorical_cols=final_categorical_cols, fold_num=fold+1, device='cuda', background_size=200, test_size=200)\n",
    "            time_2 = time.time()\n",
    "            print(f\"SHAP分析完成: {time_2-time_1} s\")\n",
    "            \n",
    "            prog_def = {'EVENT_SUBTYPE': 'IMAGING_PROGRESSION', 'VALUE_CATEGORICAL': 'Y'}\n",
    "            cf_mod = {'VALUE_CATEGORICAL': 'N'}\n",
    "            prog_events = df_full[(df_full['EVENT_SUBTYPE'] == 'IMAGING_PROGRESSION') & (df_full['VALUE_CATEGORICAL'] == 'Y')]\n",
    "            first_prog_df = prog_events.loc[prog_events.groupby('PATIENT_ID')['START_DATE'].idxmin()]\n",
    "            \n",
    "            progression_results_df = run_progression_counterfactual(\n",
    "                model=best_model, train_df_norm=train_df_norm, val_df=val_df, \n",
    "                final_first_progression_df=first_prog_df, progression_event_def=prog_def,\n",
    "                counterfactual_change=cf_mod, scaler=feature_scaler, vocabs=vocabs,\n",
    "                numerical_cols=final_numerical_cols, categorical_cols=final_categorical_cols,\n",
    "                fold_num=fold+1, device=DEVICE, n=200\n",
    "            )\n",
    "            \n",
    "            if progression_results_df is not None:\n",
    "                all_progression_results_dfs.append(progression_results_df)\n",
    "                heterogeneity_analysis_data.append({\n",
    "                    'results_df': progression_results_df,\n",
    "                    'val_df': val_df \n",
    "                })\n",
    "                model_static_features = list(set(final_numerical_cols + final_categorical_cols) - set(CORE_FEATURES))\n",
    "                # analyze_progression_impact_heterogeneity( \n",
    "                #     results_df=progression_results_df, val_df=val_df,\n",
    "                #     static_features_list=model_static_features, fold_num=fold+1\n",
    "                # )\n",
    "\n",
    "        plot_average_risk_difference_histogram(all_progression_results_dfs, SET)\n",
    "        summarize_heterogeneity_across_folds(\n",
    "            analysis_data_list=heterogeneity_analysis_data,\n",
    "            static_features_list=list(set(final_numerical_cols + final_categorical_cols) - set(CORE_FEATURES)),\n",
    "            set_name=SET\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n======================  最终交叉验证总结 ({SET.upper()})  ======================\\n{'='*80}\")\n",
    "        if final_fold_metrics:\n",
    "            results_df = pd.DataFrame(final_fold_metrics)\n",
    "            print(\"每折的最终验证集性能:\"); print(results_df.to_string(index=False))\n",
    "            print(\"\\n平均性能指标 (± 标准差):\")\n",
    "            print(f\"  - 验证集 Loss:   {results_df['val_loss'].mean():.4f} ± {results_df['val_loss'].std():.4f}\")\n",
    "            print(f\"  - 验证集 C-Index: {results_df['val_c_index'].mean():.4f} ± {results_df['val_c_index'].std():.4f}\")\n",
    "            print(f\"  - 验证集 iAUC:    {results_df['val_iauc'].mean():.4f} ± {results_df['val_iauc'].std():.4f}\")\n",
    "\n",
    "        if all_folds_td_auc:\n",
    "            combined_auc_df = pd.concat(all_folds_td_auc, ignore_index=True)\n",
    "\n",
    "            plt.figure(figsize=(10, 7))\n",
    "\n",
    "            sns.lineplot(data=combined_auc_df, x='time', y='auc', errorbar=('ci', 95), label='平均AUC (95% CI)')\n",
    "\n",
    "            total_mean_auc = combined_auc_df['auc'].mean()\n",
    "\n",
    "            plt.axhline(0.5, color='grey', linestyle='--', label='Random Guess (AUC=0.5)')\n",
    "            plt.title(f'Average Time-Dependent AUC for {SET.upper()} ({N_SPLITS}-Fold CV)\\nOverall Mean AUC = {total_mean_auc:.3f}', fontsize=15)\n",
    "            plt.xlabel('Time (Days)', fontsize=12)\n",
    "            plt.ylabel('AUC', fontsize=12)\n",
    "            plt.ylim(0.4, 1.0) \n",
    "            plt.grid(True, linestyle=':', alpha=0.6)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "\n",
    "            save_path_fig = f\"./{SET}/average_td_auc_{SET}.png\"\n",
    "            plt.savefig(save_path_fig, dpi=300)\n",
    "            plt.show()\n",
    "            print(f\"\\n平均时间依赖性AUC曲线图已保存至: {save_path_fig}\")\n",
    "\n",
    "            print(\"\\n按时间段划分的平均TD-AUC (± 标准差):\")\n",
    "\n",
    "            time_bins = [0, 365, 730, 1095, 1460, 1825]\n",
    "            time_labels = ['0-1 Year', '1-2 Years', '2-3 Years', '3-4 Years', '4-5 Years']\n",
    "\n",
    "            summary_df = combined_auc_df.copy()\n",
    "            summary_df['time_bin'] = pd.cut(summary_df['time'], bins=time_bins, labels=time_labels, right=False)\n",
    "\n",
    "            auc_summary = summary_df.groupby('time_bin')['auc'].agg(['mean', 'std']).reset_index()\n",
    "            auc_summary = auc_summary.dropna(subset=['time_bin'])\n",
    "\n",
    "            auc_summary['mean'] = auc_summary['mean'].map('{:.4f}'.format)\n",
    "            auc_summary['std'] = auc_summary['std'].map('{:.4f}'.format)\n",
    "            \n",
    "            print(auc_summary.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n脚本总执行时间: {(time.time() - time_start) / 60:.2f} 分钟\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
